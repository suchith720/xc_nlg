{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3a0f4c-1cfb-4a14-b8f2-fdea31c390ec",
   "metadata": {},
   "source": [
    "# OAK DR training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddda97e8-16ac-42c7-9ad4-f981e21c0aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp 83-oak-dr-ep-for-wikiseealso-with-renee-embedding-1-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35d8be-1323-400a-b78e-d0a4d2697801",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d685e35e-9a13-4186-b7f4-b831b10086bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import os,torch, torch.multiprocessing as mp, pickle, transformers\n",
    "from xcai.basics import *\n",
    "from xcai.models.oak import OAK001\n",
    "from xclib.utils.sparse import retain_topk\n",
    "\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c924c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_MODE'] = 'disabled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53768670-9905-46b0-9a6d-b6e91d50b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "os.environ['WANDB_PROJECT']='xc-nlg_83-oak-dr-ep-for-wikiseealso'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc96ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/scai/phd/aiz218323/Projects/XC_NLG/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14bf4ff-1e41-41f5-bd2e-7d430e7a86b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scai/phd/aiz218323/.local/lib/python3.9/site-packages/xclib-0.97-py3.9-linux-x86_64.egg/xclib/data/data_utils.py:263: UserWarning: Header mis-match from inferred shape!\n",
      "  warnings.warn(\"Header mis-match from inferred shape!\")\n"
     ]
    }
   ],
   "source": [
    "block = XCBlock.from_cfg(data_dir, 'data_linker', tfm='rm', tokenizer='distilbert-base-uncased', \n",
    "                         smp_features=[('lbl2data|lnk2lbl2data',1,1), ('lnk2data',1,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets/'\n",
    "pkl_file = f'{pkl_dir}/processed/wikiseealso_data-linker_distilbert-base-uncased_rm_oak-linker.pkl'\n",
    "\n",
    "pkl_file = f'{pkl_dir}/processed/wikiseealso_data-linker_distilbert-base-uncased_rm_oak-linker.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f70970-91ec-4301-a222-eb09760948a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a19cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "with open(pkl_file, 'rb') as file: block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddd0bc8-ee21-41b6-bf9f-675bfa2574d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "data_meta = retain_topk(block.train.dset.meta.lnk_meta.data_meta, k=10)\n",
    "block.train.dset.meta.lnk_meta.curr_data_meta = data_meta\n",
    "block.train.dset.meta.lnk_meta.data_meta = data_meta\n",
    "\n",
    "data_meta = retain_topk(block.test.dset.meta.lnk_meta.data_meta, k=3)\n",
    "block.test.dset.meta.lnk_meta.curr_data_meta = data_meta\n",
    "block.test.dset.meta.lnk_meta.data_meta = data_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc84842-2620-4508-a65e-69740975bb4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29c6ae74-3b7f-4e09-a54e-d0055b80d482",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32347f3b-e372-4ab8-8e5a-108cc82ecb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "class MultipleOptimizer(torch.optim.Optimizer):\n",
    "    # Wrapper around multiple optimizers that should be executed at the same time\n",
    "    def __init__(self, optimizers):\n",
    "        self.optimizers = optimizers\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        state = defaultdict(dict)\n",
    "        for optimizer in self.optimizers:\n",
    "            state = {**state, **optimizer.state}\n",
    "        return state\n",
    "\n",
    "    @property\n",
    "    def param_groups(self):\n",
    "        param_groups = []\n",
    "        for optimizer in self.optimizers:\n",
    "            param_groups = param_groups + optimizer.param_groups\n",
    "        return param_groups\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return [optimizer.__getstate__() for optimizer in self.optimizers]\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        for opt_state, optimizer in zip(self.optimizers, state):\n",
    "            optimizer.__setstate__(opt_state)\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + ' ('\n",
    "        for optimizer in self.optimizers:\n",
    "            format_string += '\\n'\n",
    "            format_string += optimizer.__repr__()\n",
    "        format_string += ')'\n",
    "        return format_string\n",
    "\n",
    "    def _hook_for_profile(self):\n",
    "        for optimizer in self.optimizers:\n",
    "            optimizer._hook_for_profile()\n",
    "\n",
    "    def state_dict(self):\n",
    "        return [optimizer.state_dict() for optimizer in self.optimizers]\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        for state, optimizer in zip(state_dict, self.optimizers):\n",
    "            optimizer.load_state_dict(state)\n",
    "\n",
    "    def zero_grad(self, set_to_none: bool = False):\n",
    "        for optimizer in self.optimizers:\n",
    "            optimizer.zero_grad(set_to_none=set_to_none)\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for optimizer in self.optimizers:\n",
    "            optimizer.step()\n",
    "\n",
    "        return loss\n",
    "        \n",
    "\n",
    "class MultipleScheduler(object):\n",
    "    \n",
    "    def __init__(self, sched):\n",
    "        self.schedulers = sched\n",
    "    \n",
    "    def step(self, *args, **kwargs):\n",
    "        for sched in self.schedulers: sched.step(*args, **kwargs)\n",
    "\n",
    "    def get_last_lr(self):\n",
    "        return list(chain(*[s.get_last_lr() for s in self.schedulers]))\n",
    "\n",
    "    def state_dict(self):\n",
    "        return [sched.state_dict() for sched in self.schedulers]\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        for sched,state in zip(self.schedulers, state_dict):\n",
    "            sched.load_state_dict(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bf5d1e-7e21-419e-83b3-c8f66c3f5f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179d9be4-250c-469e-ad40-8575eb4da23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def create_optimizer_and_scheduler(self:XCLearner, num_training_steps: int):\n",
    "    NO_DECAY = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "    dense, sparse = [], []\n",
    "    for k, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            if \"meta_embeddings\" not in k: dense.append((k,p))\n",
    "            else: sparse.append(p)\n",
    "                \n",
    "    params = [\n",
    "        {'params': [p for n, p in dense if not any(nd in n for nd in NO_DECAY)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in dense if any(nd in n for nd in NO_DECAY)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "\n",
    "    optimizer_list = [torch.optim.AdamW(params, **{'lr': self.args.learning_rate, 'eps': 1e-6}),\n",
    "                      torch.optim.SparseAdam(sparse, **{'lr': self.args.learning_rate * self.args.free_parameter_lr_coefficient, 'eps': 1e-6})]\n",
    "    \n",
    "    self.optimizer = MultipleOptimizer(optimizer_list)\n",
    "    scheduler_list = [transformers.get_linear_schedule_with_warmup(self.optimizer.optimizers[0], num_warmup_steps=self.args.warmup_steps, \n",
    "                                                                   num_training_steps=num_training_steps),\n",
    "                        transformers.get_cosine_schedule_with_warmup(self.optimizer.optimizers[1], \n",
    "                                                                     num_warmup_steps=self.args.free_parameter_warmup_steps, \n",
    "                                                                     num_training_steps=num_training_steps)]\n",
    "    \n",
    "    self.lr_scheduler = MultipleScheduler(scheduler_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2554487b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521c4ff-94e8-43a2-84eb-553610cb7de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "args = XCLearningArguments(\n",
    "    output_dir='/home/scai/phd/aiz218323/scratch/outputs/83-oak-dr-ep-for-wikiseealso-with-renee-embedding-1-0',\n",
    "    logging_first_step=True,\n",
    "    per_device_train_batch_size=800,\n",
    "    per_device_eval_batch_size=800,\n",
    "    representation_num_beams=200,\n",
    "    representation_accumulation_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5000,\n",
    "    save_steps=5000,\n",
    "    save_total_limit=5,\n",
    "    num_train_epochs=300,\n",
    "    predict_with_representation=True,\n",
    "    adam_epsilon=1e-6,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-4,\n",
    "    generation_num_beams=10,\n",
    "    generation_length_penalty=1.5,\n",
    "    predict_with_generation=True,\n",
    "    representation_search_type='BRUTEFORCE',\n",
    "    \n",
    "    output_representation_attribute='data_fused_repr',\n",
    "    label_representation_attribute='data_repr',\n",
    "    metadata_representation_attribute='data_repr',\n",
    "    data_augmentation_attribute='data_repr',\n",
    "    representation_attribute='data_fused_repr',\n",
    "    clustering_representation_attribute='data_fused_repr',\n",
    "    \n",
    "    group_by_cluster=True,\n",
    "    num_clustering_warmup_epochs=10,\n",
    "    num_cluster_update_epochs=5,\n",
    "    num_cluster_size_update_epochs=25,\n",
    "    use_data_metadata_for_clustering=True,\n",
    "    clustering_type='EXPO',\n",
    "    minimum_cluster_size=2,\n",
    "    maximum_cluster_size=1600,\n",
    "\n",
    "    metric_for_best_model='P@1',\n",
    "    load_best_model_at_end=True,\n",
    "    target_indices_key='plbl2data_idx',\n",
    "    target_pointer_key='plbl2data_data2ptr',\n",
    "    \n",
    "    use_distributional_representation=False,\n",
    "    use_encoder_parallel=True,\n",
    "    max_grad_norm=None, \n",
    "    fp16=True,\n",
    "    \n",
    "    label_names=['lnk2data_idx', 'lnk2data_input_ids', 'lnk2data_attention_mask'],\n",
    "    \n",
    "    prune_metadata=False,\n",
    "    num_metadata_prune_warmup_epochs=10,\n",
    "    num_metadata_prune_epochs=5,\n",
    "    metadata_prune_batch_size=2048,\n",
    "    prune_metadata_names=['cat_meta'],\n",
    "    use_data_metadata_for_pruning=True,\n",
    "\n",
    "    predict_with_augmentation=False,\n",
    "    use_augmentation_index_representation=True,\n",
    "    \n",
    "    data_aug_meta_name='lnk',\n",
    "    augmentation_num_beams=3,\n",
    "    data_aug_prefix='lnk',\n",
    "    use_label_metadata=False,\n",
    "    \n",
    "    data_meta_batch_size=2048,\n",
    "    augment_metadata=False,\n",
    "    num_metadata_augment_warmup_epochs=10,\n",
    "    num_metadata_augment_epochs=5,\n",
    "\n",
    "    free_parameter_lr_coefficient=1000,\n",
    "    free_parameter_warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "metric = PrecRecl(block.n_lbl, block.test.data_lbl_filterer, prop=block.train.dset.data.data_lbl,\n",
    "                  pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29041ca-d9ac-4607-a053-263390cb6bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12171f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OAK001 were not initialized from the model checkpoint at sentence-transformers/msmarco-distilbert-base-v4 and are newly initialized: ['encoder.cross_head.k.bias', 'encoder.cross_head.k.weight', 'encoder.cross_head.o.bias', 'encoder.cross_head.o.weight', 'encoder.cross_head.q.bias', 'encoder.cross_head.q.weight', 'encoder.cross_head.v.bias', 'encoder.cross_head.v.weight', 'encoder.dr_fused_head.layer_norm.bias', 'encoder.dr_fused_head.layer_norm.weight', 'encoder.dr_fused_head.projector.bias', 'encoder.dr_fused_head.projector.weight', 'encoder.dr_fused_head.transform.bias', 'encoder.dr_fused_head.transform.weight', 'encoder.dr_head.layer_norm.bias', 'encoder.dr_head.layer_norm.weight', 'encoder.dr_head.projector.bias', 'encoder.dr_head.projector.weight', 'encoder.dr_head.transform.bias', 'encoder.dr_head.transform.weight', 'encoder.meta_embeddings.weight', 'encoder.meta_head.layer_norm.bias', 'encoder.meta_head.layer_norm.weight', 'encoder.meta_head.projector.bias', 'encoder.meta_head.projector.weight', 'encoder.meta_head.transform.bias', 'encoder.meta_head.transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()\n",
    "\n",
    "model = OAK001.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', batch_size=bsz, num_batch_labels=5000, \n",
    "                               margin=0.3, num_negatives=5, tau=0.1, apply_softmax=True,\n",
    "                               \n",
    "                               data_aug_meta_prefix='lnk2data', lbl2data_aug_meta_prefix=None, \n",
    "                               data_pred_meta_prefix=None, lbl2data_pred_meta_prefix=None,\n",
    "                               \n",
    "                               num_metadata=block.train.dset.meta['lnk_meta'].n_meta, resize_length=5000,\n",
    "                               \n",
    "                               calib_margin=0.3, calib_num_negatives=10, calib_tau=0.1, calib_apply_softmax=False, \n",
    "                               calib_loss_weight=0.1, use_calib_loss=True,\n",
    "\n",
    "                               use_query_loss=True,\n",
    "\n",
    "                               meta_loss_weight=0.0, \n",
    "                               \n",
    "                               fusion_loss_weight=0.1, use_fusion_loss=False,\n",
    "                               \n",
    "                               use_encoder_parallel=False)\n",
    "\n",
    "model.init_retrieval_head()\n",
    "model.init_cross_head()\n",
    "\n",
    "model.encoder.set_meta_embeddings(torch.zeros(block.train.dset.meta['lnk_meta'].n_meta, model.config.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d48f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "learn = XCLearner(\n",
    "    model=model, \n",
    "    args=args,\n",
    "    train_dataset=block.train.dset,\n",
    "    eval_dataset=block.test.dset,\n",
    "    data_collator=block.collator,\n",
    "    compute_metrics=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f7a9e-5371-471f-927a-ad014b7c3e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func():\n",
    "    import pdb; pdb.set_trace()\n",
    "    learn.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee66d844-b9c7-4a05-95b6-1e1632faab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.distilbert.embeddings.word_embeddings.weight', 'encoder.distilbert.embeddings.position_embeddings.weight', 'encoder.distilbert.embeddings.LayerNorm.weight', 'encoder.distilbert.embeddings.LayerNorm.bias', 'encoder.distilbert.transformer.layer.0.attention.q_lin.weight', 'encoder.distilbert.transformer.layer.0.attention.q_lin.bias', 'encoder.distilbert.transformer.layer.0.attention.k_lin.weight', 'encoder.distilbert.transformer.layer.0.attention.k_lin.bias', 'encoder.distilbert.transformer.layer.0.attention.v_lin.weight', 'encoder.distilbert.transformer.layer.0.attention.v_lin.bias', 'encoder.distilbert.transformer.layer.0.attention.out_lin.weight', 'encoder.distilbert.transformer.layer.0.attention.out_lin.bias', 'encoder.distilbert.transformer.layer.0.sa_layer_norm.weight', 'encoder.distilbert.transformer.layer.0.sa_layer_norm.bias', 'encoder.distilbert.transformer.layer.0.ffn.lin1.weight', 'encoder.distilbert.transformer.layer.0.ffn.lin1.bias', 'encoder.distilbert.transformer.layer.0.ffn.lin2.weight', 'encoder.distilbert.transformer.layer.0.ffn.lin2.bias', 'encoder.distilbert.transformer.layer.0.output_layer_norm.weight', 'encoder.distilbert.transformer.layer.0.output_layer_norm.bias', 'encoder.distilbert.transformer.layer.1.attention.q_lin.weight', 'encoder.distilbert.transformer.layer.1.attention.q_lin.bias', 'encoder.distilbert.transformer.layer.1.attention.k_lin.weight', 'encoder.distilbert.transformer.layer.1.attention.k_lin.bias', 'encoder.distilbert.transformer.layer.1.attention.v_lin.weight', 'encoder.distilbert.transformer.layer.1.attention.v_lin.bias', 'encoder.distilbert.transformer.layer.1.attention.out_lin.weight', 'encoder.distilbert.transformer.layer.1.attention.out_lin.bias', 'encoder.distilbert.transformer.layer.1.sa_layer_norm.weight', 'encoder.distilbert.transformer.layer.1.sa_layer_norm.bias', 'encoder.distilbert.transformer.layer.1.ffn.lin1.weight', 'encoder.distilbert.transformer.layer.1.ffn.lin1.bias', 'encoder.distilbert.transformer.layer.1.ffn.lin2.weight', 'encoder.distilbert.transformer.layer.1.ffn.lin2.bias', 'encoder.distilbert.transformer.layer.1.output_layer_norm.weight', 'encoder.distilbert.transformer.layer.1.output_layer_norm.bias', 'encoder.distilbert.transformer.layer.2.attention.q_lin.weight', 'encoder.distilbert.transformer.layer.2.attention.q_lin.bias', 'encoder.distilbert.transformer.layer.2.attention.k_lin.weight', 'encoder.distilbert.transformer.layer.2.attention.k_lin.bias', 'encoder.distilbert.transformer.layer.2.attention.v_lin.weight', 'encoder.distilbert.transformer.layer.2.attention.v_lin.bias', 'encoder.distilbert.transformer.layer.2.attention.out_lin.weight', 'encoder.distilbert.transformer.layer.2.attention.out_lin.bias', 'encoder.distilbert.transformer.layer.2.sa_layer_norm.weight', 'encoder.distilbert.transformer.layer.2.sa_layer_norm.bias', 'encoder.distilbert.transformer.layer.2.ffn.lin1.weight', 'encoder.distilbert.transformer.layer.2.ffn.lin1.bias', 'encoder.distilbert.transformer.layer.2.ffn.lin2.weight', 'encoder.distilbert.transformer.layer.2.ffn.lin2.bias', 'encoder.distilbert.transformer.layer.2.output_layer_norm.weight', 'encoder.distilbert.transformer.layer.2.output_layer_norm.bias', 'encoder.distilbert.transformer.layer.3.attention.q_lin.weight', 'encoder.distilbert.transformer.layer.3.attention.q_lin.bias', 'encoder.distilbert.transformer.layer.3.attention.k_lin.weight', 'encoder.distilbert.transformer.layer.3.attention.k_lin.bias', 'encoder.distilbert.transformer.layer.3.attention.v_lin.weight', 'encoder.distilbert.transformer.layer.3.attention.v_lin.bias', 'encoder.distilbert.transformer.layer.3.attention.out_lin.weight', 'encoder.distilbert.transformer.layer.3.attention.out_lin.bias', 'encoder.distilbert.transformer.layer.3.sa_layer_norm.weight', 'encoder.distilbert.transformer.layer.3.sa_layer_norm.bias', 'encoder.distilbert.transformer.layer.3.ffn.lin1.weight', 'encoder.distilbert.transformer.layer.3.ffn.lin1.bias', 'encoder.distilbert.transformer.layer.3.ffn.lin2.weight', 'encoder.distilbert.transformer.layer.3.ffn.lin2.bias', 'encoder.distilbert.transformer.layer.3.output_layer_norm.weight', 'encoder.distilbert.transformer.layer.3.output_layer_norm.bias', 'encoder.distilbert.transformer.layer.4.attention.q_lin.weight', 'encoder.distilbert.transformer.layer.4.attention.q_lin.bias', 'encoder.distilbert.transformer.layer.4.attention.k_lin.weight', 'encoder.distilbert.transformer.layer.4.attention.k_lin.bias', 'encoder.distilbert.transformer.layer.4.attention.v_lin.weight', 'encoder.distilbert.transformer.layer.4.attention.v_lin.bias', 'encoder.distilbert.transformer.layer.4.attention.out_lin.weight', 'encoder.distilbert.transformer.layer.4.attention.out_lin.bias', 'encoder.distilbert.transformer.layer.4.sa_layer_norm.weight', 'encoder.distilbert.transformer.layer.4.sa_layer_norm.bias', 'encoder.distilbert.transformer.layer.4.ffn.lin1.weight', 'encoder.distilbert.transformer.layer.4.ffn.lin1.bias', 'encoder.distilbert.transformer.layer.4.ffn.lin2.weight', 'encoder.distilbert.transformer.layer.4.ffn.lin2.bias', 'encoder.distilbert.transformer.layer.4.output_layer_norm.weight', 'encoder.distilbert.transformer.layer.4.output_layer_norm.bias', 'encoder.distilbert.transformer.layer.5.attention.q_lin.weight', 'encoder.distilbert.transformer.layer.5.attention.q_lin.bias', 'encoder.distilbert.transformer.layer.5.attention.k_lin.weight', 'encoder.distilbert.transformer.layer.5.attention.k_lin.bias', 'encoder.distilbert.transformer.layer.5.attention.v_lin.weight', 'encoder.distilbert.transformer.layer.5.attention.v_lin.bias', 'encoder.distilbert.transformer.layer.5.attention.out_lin.weight', 'encoder.distilbert.transformer.layer.5.attention.out_lin.bias', 'encoder.distilbert.transformer.layer.5.sa_layer_norm.weight', 'encoder.distilbert.transformer.layer.5.sa_layer_norm.bias', 'encoder.distilbert.transformer.layer.5.ffn.lin1.weight', 'encoder.distilbert.transformer.layer.5.ffn.lin1.bias', 'encoder.distilbert.transformer.layer.5.ffn.lin2.weight', 'encoder.distilbert.transformer.layer.5.ffn.lin2.bias', 'encoder.distilbert.transformer.layer.5.output_layer_norm.weight', 'encoder.distilbert.transformer.layer.5.output_layer_norm.bias'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='10396500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [      21/10396500 00:00 < 196:45:16, 14.68 it/s, Epoch 0.00/300]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@10</th>\n",
       "      <th>P@3</th>\n",
       "      <th>P@5</th>\n",
       "      <th>N@1</th>\n",
       "      <th>N@10</th>\n",
       "      <th>N@3</th>\n",
       "      <th>N@5</th>\n",
       "      <th>Psp@1</th>\n",
       "      <th>Psp@10</th>\n",
       "      <th>Psp@3</th>\n",
       "      <th>Psp@5</th>\n",
       "      <th>Psn@1</th>\n",
       "      <th>Psn@10</th>\n",
       "      <th>Psn@3</th>\n",
       "      <th>Psn@5</th>\n",
       "      <th>R@200</th>\n",
       "      <th>R@10</th>\n",
       "      <th>R@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.173646</td>\n",
       "      <td>0.165969</td>\n",
       "      <td>0.055613</td>\n",
       "      <td>0.110689</td>\n",
       "      <td>0.084700</td>\n",
       "      <td>0.165969</td>\n",
       "      <td>0.187551</td>\n",
       "      <td>0.165420</td>\n",
       "      <td>0.173109</td>\n",
       "      <td>0.152268</td>\n",
       "      <td>0.201918</td>\n",
       "      <td>0.159432</td>\n",
       "      <td>0.172105</td>\n",
       "      <td>0.152268</td>\n",
       "      <td>0.190223</td>\n",
       "      <td>0.162849</td>\n",
       "      <td>0.173985</td>\n",
       "      <td>0.437756</td>\n",
       "      <td>0.229009</td>\n",
       "      <td>0.387538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25a0d57c9654d03ab5dc0c18b4aee36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/scipy/sparse/_index.py:145: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/scai/phd/aiz218323/.local/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a679289a-231a-428c-bac8-7f4cf449ce07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cd155e-912d-4780-b60d-b7c6749b30b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_16183/1403786160.py(17)create_optimizer_and_scheduler()\n",
      "     15 \n",
      "     16     import pdb; pdb.set_trace()\n",
      "---> 17     optimizer_list = [torch.optim.AdamW(params, **{'lr': self.args.learning_rate, 'eps': 1e-6}),\n",
      "     18                       torch.optim.SparseAdam(sparse, **{'lr': self.args.learning_rate * self.args.free_parameter_lr_coefficient, 'eps': 1e-6})]\n",
      "     19 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "if __name__ == '__main__':\n",
    "    mp.freeze_support()\n",
    "    learn.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130617dd-f34b-41f8-a299-3a2654c8939a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5df2b60-c20b-4e7c-83fe-d583e55d781c",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d622383-db42-403d-8b0c-e2d44f9bfad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = XCLearningArguments(\n",
    "    output_dir='/home/scai/phd/aiz218323/scratch/outputs/72-radga-dr-ep-for-wikiseealso-1-0',\n",
    "    logging_first_step=True,\n",
    "    per_device_train_batch_size=800,\n",
    "    per_device_eval_batch_size=800,\n",
    "    representation_num_beams=200,\n",
    "    representation_accumulation_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5000,\n",
    "    save_steps=5000,\n",
    "    save_total_limit=5,\n",
    "    num_train_epochs=300,\n",
    "    predict_with_representation=True,\n",
    "    adam_epsilon=1e-6,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-4,\n",
    "    generation_num_beams=10,\n",
    "    generation_length_penalty=1.5,\n",
    "    predict_with_generation=True,\n",
    "    representation_search_type='INDEX',\n",
    "    \n",
    "    output_representation_attribute='data_fused_repr',\n",
    "    label_representation_attribute='data_repr',\n",
    "    metadata_representation_attribute='data_repr',\n",
    "    data_augmentation_attribute='data_repr',\n",
    "    representation_attribute='data_repr',\n",
    "    clustering_representation_attribute='data_fused_repr',\n",
    "    \n",
    "    group_by_cluster=True,\n",
    "    num_clustering_warmup_epochs=10,\n",
    "    num_cluster_update_epochs=5,\n",
    "    num_cluster_size_update_epochs=25,\n",
    "    use_data_metadata_for_clustering=True,\n",
    "    clustering_type='EXPO',\n",
    "    minimum_cluster_size=2,\n",
    "    maximum_cluster_size=1600,\n",
    "\n",
    "    metric_for_best_model='P@1',\n",
    "    load_best_model_at_end=True,\n",
    "    target_indices_key='plbl2data_idx',\n",
    "    target_pointer_key='plbl2data_data2ptr',\n",
    "    \n",
    "    use_distributional_representation=False,\n",
    "    use_encoder_parallel=True,\n",
    "    max_grad_norm=None, \n",
    "    fp16=True,\n",
    "    \n",
    "    label_names=['cat2data_idx', 'cat2data_input_ids', 'cat2data_attention_mask'],\n",
    "\n",
    "    prune_metadata=False,\n",
    "    num_metadata_prune_warmup_epochs=10,\n",
    "    num_metadata_prune_epochs=5,\n",
    "    metadata_prune_batch_size=2048,\n",
    "    prune_metadata_names=['cat_meta'],\n",
    "    use_data_metadata_for_pruning=True,\n",
    "\n",
    "    predict_with_augmentation=False,\n",
    "    use_augmentation_index_representation=True,\n",
    "    \n",
    "    data_aug_meta_name='cat',\n",
    "    augmentation_num_beams=3,\n",
    "    data_aug_prefix='cat',\n",
    "    use_label_metadata=False,\n",
    "    \n",
    "    data_meta_batch_size=2048,\n",
    "    augment_metadata=False,\n",
    "    num_metadata_augment_warmup_epochs=10,\n",
    "    num_metadata_augment_epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4865c84b-0d78-47ea-ac8a-ed10a324e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = PrecRecl(block.n_lbl, block.test.data_lbl_filterer, prop=block.train.dset.data.data_lbl,\n",
    "                  pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69926870-0db1-4bc8-9117-a9b16983144d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RAD002 were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.cross_head.k.bias', 'encoder.cross_head.k.weight', 'encoder.cross_head.o.bias', 'encoder.cross_head.o.weight', 'encoder.cross_head.q.bias', 'encoder.cross_head.q.weight', 'encoder.cross_head.v.bias', 'encoder.cross_head.v.weight', 'encoder.dr_head.layer_norm.bias', 'encoder.dr_head.layer_norm.weight', 'encoder.dr_head.projector.bias', 'encoder.dr_head.projector.weight', 'encoder.dr_head.transform.bias', 'encoder.dr_head.transform.weight', 'encoder.meta_head.layer_norm.bias', 'encoder.meta_head.layer_norm.weight', 'encoder.meta_head.projector.bias', 'encoder.meta_head.projector.weight', 'encoder.meta_head.transform.bias', 'encoder.meta_head.transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()\n",
    "\n",
    "model = RAD002.from_pretrained('distilbert-base-uncased', num_batch_labels=5000, batch_size=bsz,\n",
    "                               margin=0.3, num_negatives=5, tau=0.1, apply_softmax=True,\n",
    "                               \n",
    "                               data_aug_meta_prefix='cat2data', lbl2data_aug_meta_prefix=None, \n",
    "                               data_pred_meta_prefix=None, lbl2data_pred_meta_prefix=None,\n",
    "                               \n",
    "                               resize_length=5000, use_noise=True, noise_percent=0.5,\n",
    "                               \n",
    "                               meta_loss_weight=0.3, fusion_loss_weight=0.1, \n",
    "                               use_fusion_loss=False,  use_encoder_parallel=True)\n",
    "\n",
    "model.init_retrieval_head()\n",
    "model.init_cross_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4f1045-de24-4be9-890b-defb99f40133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5329ca-97c0-4560-b097-cf0d5d11cf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "learn = XCLearner(\n",
    "    model=model, \n",
    "    args=args,\n",
    "    train_dataset=block.train.dset,\n",
    "    eval_dataset=block.test.dset,\n",
    "    data_collator=block.collator,\n",
    "    compute_metrics=metric,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
