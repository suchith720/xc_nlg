{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3a0f4c-1cfb-4a14-b8f2-fdea31c390ec",
   "metadata": {},
   "source": [
    "# Ramen Clover training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddda97e8-16ac-42c7-9ad4-f981e21c0aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp 51-encoder-parallel-ramen-clover-for-wikititles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f35d8be-1323-400a-b78e-d0a4d2697801",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d685e35e-9a13-4186-b7f4-b831b10086bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os,torch, torch.multiprocessing as mp, pickle\n",
    "from xcai.basics import *\n",
    "from xcai.models.PPP0XX import DBT017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0c924c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_MODE'] = 'disabled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53768670-9905-46b0-9a6d-b6e91d50b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5'\n",
    "os.environ['WANDB_PROJECT']='xc-nlg_36-ramen-style-oak-training-pipeline-with-multitriplet-loss-with-clustering-for-wikititles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dc96ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "data_dir = '/home/aiscuser/scratch/datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba38fcb0-3ebf-4f79-aa20-1a1321b791c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiscuser/.local/lib/python3.9/site-packages/xclib-0.97-py3.9-linux-x86_64.egg/xclib/data/data_utils.py:263: UserWarning: Header mis-match from inferred shape!\n",
      "  warnings.warn(\"Header mis-match from inferred shape!\")\n"
     ]
    }
   ],
   "source": [
    "block = XCBlock.from_cfg(data_dir, 'data_meta', dset='wikititles', valid_pct=0.001, tfm='rm', tokenizer='distilbert-base-uncased', \n",
    "                         smp_features=[('lbl2data|sal2lbl2data', 1, 1), ('sal2data', 1, 1)],\n",
    "                         n_data_meta_samples=50, n_lbl_meta_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67f7ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "pkl_dir = f'{data_dir}/processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b237c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{pkl_dir}/wikititles_data-meta_distilbert-base-uncased_rm_ramen-sal-encoder-parallel.pkl', 'wb') as file: \n",
    "    pickle.dump(block, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a19cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "with open(f'{pkl_dir}/wikititles_data-meta_distilbert-base-uncased_rm_ramen-sal-encoder-parallel.pkl', 'rb') as file: \n",
    "    block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe9fa20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73b6a059-3409-4c00-a575-dcdb5775adc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "args = XCLearningArguments(\n",
    "    output_dir='/home/aiscuser/outputs/51-encoder-parallel-ramen-clover-for-wikititles-1-0',\n",
    "    logging_first_step=True,\n",
    "    per_device_train_batch_size=600,\n",
    "    per_device_eval_batch_size=100,\n",
    "    representation_num_beams=200,\n",
    "    representation_accumulation_steps=1,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=5,\n",
    "    num_train_epochs=300,\n",
    "    predict_with_representation=True,\n",
    "    adam_epsilon=1e-6,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-4,\n",
    "    generation_num_beams=10,\n",
    "    generation_length_penalty=1.5,\n",
    "    predict_with_generation=True,\n",
    "    representation_search_type='INDEX',\n",
    "    group_by_cluster=True,\n",
    "    num_clustering_warmup_epochs=10,\n",
    "    num_cluster_update_epochs=5,\n",
    "    num_cluster_size_update_epochs=10,\n",
    "    clustering_type='EXPO',\n",
    "    minimum_cluster_size=1,\n",
    "    maximum_cluster_size=300,\n",
    "    output_concatenation_weight=1.0,\n",
    "    metric_for_best_model='P@1',\n",
    "    load_best_model_at_end=True,\n",
    "    target_indices_key='plbl2data_idx',\n",
    "    target_pointer_key='plbl2data_data2ptr',\n",
    "    fp16=True,\n",
    "    label_names=['sal2data_idx', 'sal2data_input_ids', 'sal2data_attention_mask',\n",
    "                 'sal2lbl2data_idx', 'sal2lbl2data_input_ids', 'sal2lbl2data_attention_mask'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfbc728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "test_dset = block.test.dset.sample(n=2000, seed=50)\n",
    "metric = PrecRecl(block.n_lbl, test_dset.data.data_lbl_filterer, prop=block.train.dset.data.data_lbl,\n",
    "                  pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a12171f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT017 were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.module.dr_layer_norm.bias', 'encoder.module.dr_layer_norm.weight', 'encoder.module.dr_projector.bias', 'encoder.module.dr_projector.weight', 'encoder.module.dr_transform.bias', 'encoder.module.dr_transform.weight', 'encoder.module.vocab_projector.weight', 'vocab_projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()\n",
    "\n",
    "model = DBT017.from_pretrained('distilbert-base-uncased', ig_tok=0, bsz=bsz, tn_targ=5000, margin=0.3, tau=0.1, \n",
    "                               n_negatives=5, apply_softmax=True, lw=0.001, m_lw=0.3, meta_prefix='sal', \n",
    "                               tie_word_embeddings=False)\n",
    "model.init_dr_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c46e67e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087f14a2bc6a43ab8205ba34422e86d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/501070 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| export\n",
    "trie = XCTrie.from_block(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01d48f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "learn = XCLearner(\n",
    "    model=model, \n",
    "    args=args,\n",
    "    trie=trie,\n",
    "    train_dataset=block.train.dset,\n",
    "    eval_dataset=test_dset,\n",
    "    data_collator=block.collator,\n",
    "    compute_metrics=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aba00769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-31 21:45:57,227] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node-0:2603038:2603038 [0] NCCL INFO Bootstrap : Using eth0:10.13.60.215<0>\n",
      "node-0:2603038:2603038 [0] NCCL INFO NET/Plugin : Plugin load (librccl-net.so) returned 2 : librccl-net.so: cannot open shared object file: No such file or directory\n",
      "node-0:2603038:2603038 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation\n",
      "node-0:2603038:2603038 [0] NCCL INFO Kernel version: 5.15.0-1042-azure\n",
      "RCCL version 2.17.1+hip5.7 HEAD:cbbb3d8+\n",
      "\n",
      "node-0:2603038:2605074 [0] /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/rccl/build/hipify/src/misc/ibvwrap.cc:222 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2603038:2605074 [0] /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/rccl/build/hipify/src/transport/net_ib.cc:199 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:2603038:2605074 [0] /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/rccl/build/hipify/src/misc/ibvwrap.cc:222 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2603038:2605074 [0] /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/rccl/build/hipify/src/transport/net_ib.cc:199 NCCL WARN NET/IB : Unable to open device mlx5_1\n",
      "\n",
      "node-0:2603038:2605074 [0] /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/rccl/build/hipify/src/misc/ibvwrap.cc:222 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2603038:2605074 [0] /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/rccl/build/hipify/src/transport/net_ib.cc:199 NCCL WARN NET/IB : Unable to open device mlx5_2\n",
      "\n",
      "node-0:2603038:2605074 [0] /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/rccl/build/hipify/src/misc/ibvwrap.cc:222 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2603038:2605074 [0] /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/rccl/build/hipify/src/transport/net_ib.cc:199 NCCL WARN NET/IB : Unable to open device mlx5_3\n",
      "\n",
      "node-0:2603038:2605074 [0] /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/rccl/build/hipify/src/misc/ibvwrap.cc:222 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2603038:2605074 [0] /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/rccl/build/hipify/src/transport/net_ib.cc:199 NCCL WARN NET/IB : Unable to open device mlx5_4\n",
      "\n",
      "node-0:2603038:2605074 [0] /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/rccl/build/hipify/src/misc/ibvwrap.cc:222 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2603038:2605074 [0] /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/rccl/build/hipify/src/transport/net_ib.cc:199 NCCL WARN NET/IB : Unable to open device mlx5_5\n",
      "\n",
      "node-0:2603038:2605074 [0] /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/rccl/build/hipify/src/misc/ibvwrap.cc:222 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2603038:2605074 [0] /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/rccl/build/hipify/src/transport/net_ib.cc:199 NCCL WARN NET/IB : Unable to open device mlx5_6\n",
      "\n",
      "node-0:2603038:2605074 [0] /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/rccl/build/hipify/src/misc/ibvwrap.cc:222 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2603038:2605074 [0] /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/rccl/build/hipify/src/transport/net_ib.cc:199 NCCL WARN NET/IB : Unable to open device mlx5_7\n",
      "\n",
      "node-0:2603038:2605074 [0] /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/rccl/build/hipify/src/misc/ibvwrap.cc:222 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2603038:2605074 [0] /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/rccl/build/hipify/src/transport/net_ib.cc:199 NCCL WARN NET/IB : Unable to open device mlx5_8\n",
      "node-0:2603038:2605074 [0] NCCL INFO NET/IB : No device found.\n",
      "node-0:2603038:2605074 [0] NCCL INFO NET/Socket : Using [0]eth0:10.13.60.215<0>\n",
      "node-0:2603038:2605074 [0] NCCL INFO Using network Socket\n",
      "node-0:2603038:2605075 [1] NCCL INFO Using network Socket\n",
      "node-0:2603038:2605074 [0] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/47505500-0009-0000-3130-303237343043/pci0009:00/0009:00:00.0/../max_link_speed, ignoring\n",
      "node-0:2603038:2605075 [1] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/47505500-0009-0000-3130-303237343043/pci0009:00/0009:00:00.0/../max_link_speed, ignoring\n",
      "node-0:2603038:2605074 [0] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/47505500-0009-0000-3130-303237343043/pci0009:00/0009:00:00.0/../max_link_width, ignoring\n",
      "node-0:2603038:2605075 [1] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/47505500-0009-0000-3130-303237343043/pci0009:00/0009:00:00.0/../max_link_width, ignoring\n",
      "node-0:2603038:2605074 [0] NCCL INFO rocm_smi_lib: version 5.0.0.0\n",
      "node-0:2603038:2605075 [1] NCCL INFO rocm_smi_lib: version 5.0.0.0\n",
      "node-0:2603038:2605074 [0] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/47505500-000a-0000-3130-303237343043/pci000a:00/000a:00:00.0/../max_link_speed, ignoring\n",
      "node-0:2603038:2605075 [1] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/47505500-000a-0000-3130-303237343043/pci000a:00/000a:00:00.0/../max_link_speed, ignoring\n",
      "node-0:2603038:2605074 [0] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/47505500-000a-0000-3130-303237343043/pci000a:00/000a:00:00.0/../max_link_width, ignoring\n",
      "node-0:2603038:2605075 [1] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/47505500-000a-0000-3130-303237343043/pci000a:00/000a:00:00.0/../max_link_width, ignoring\n",
      "node-0:2603038:2605074 [0] NCCL INFO === System : maxBw 144.0 totalBw 144.0 ===\n",
      "node-0:2603038:2605074 [0] NCCL INFO CPU/3 (1/2/4)\n",
      "node-0:2603038:2605074 [0] NCCL INFO + PCI[5000.0] - NIC/0\n",
      "node-0:2603038:2605074 [0] NCCL INFO + PCI[24.0] - GPU/900000 (0)\n",
      "node-0:2603038:2605074 [0] NCCL INFO               + XGMI[144.0] - GPU/A00000\n",
      "node-0:2603038:2605074 [0] NCCL INFO + PCI[24.0] - GPU/A00000 (1)\n",
      "node-0:2603038:2605074 [0] NCCL INFO               + XGMI[144.0] - GPU/900000\n",
      "node-0:2603038:2605074 [0] NCCL INFO ==========================================\n",
      "node-0:2603038:2605074 [0] NCCL INFO GPU/900000 :GPU/900000 (0/5000.000000/LOC) GPU/A00000 (1/144.000000/XGMI) CPU/3 (1/24.000000/PHB) \n",
      "node-0:2603038:2605074 [0] NCCL INFO GPU/A00000 :GPU/900000 (1/144.000000/XGMI) GPU/A00000 (0/5000.000000/LOC) CPU/3 (1/24.000000/PHB) \n",
      "node-0:2603038:2605074 [0] NCCL INFO Setting affinity for GPU 8 to ffffff00,00000000,00000000\n",
      "node-0:2603038:2605075 [1] NCCL INFO === System : maxBw 144.0 totalBw 144.0 ===\n",
      "node-0:2603038:2605075 [1] NCCL INFO CPU/3 (1/2/4)\n",
      "node-0:2603038:2605075 [1] NCCL INFO + PCI[5000.0] - NIC/0\n",
      "node-0:2603038:2605075 [1] NCCL INFO + PCI[24.0] - GPU/900000 (0)\n",
      "node-0:2603038:2605075 [1] NCCL INFO               + XGMI[144.0] - GPU/A00000\n",
      "node-0:2603038:2605075 [1] NCCL INFO + PCI[24.0] - GPU/A00000 (1)\n",
      "node-0:2603038:2605075 [1] NCCL INFO               + XGMI[144.0] - GPU/900000\n",
      "node-0:2603038:2605075 [1] NCCL INFO ==========================================\n",
      "node-0:2603038:2605075 [1] NCCL INFO GPU/900000 :GPU/900000 (0/5000.000000/LOC) GPU/A00000 (1/144.000000/XGMI) CPU/3 (1/24.000000/PHB) \n",
      "node-0:2603038:2605075 [1] NCCL INFO GPU/A00000 :GPU/900000 (1/144.000000/XGMI) GPU/A00000 (0/5000.000000/LOC) CPU/3 (1/24.000000/PHB) \n",
      "node-0:2603038:2605075 [1] NCCL INFO Setting affinity for GPU 9 to ffffff00,00000000,00000000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 6, bw 24.000000/24.000000, type XGMI/PIX, sameChannels 1\n",
      "node-0:2603038:2605074 [0] NCCL INFO  0 : GPU/0 GPU/1\n",
      "node-0:2603038:2605074 [0] NCCL INFO  1 : GPU/0 GPU/1\n",
      "node-0:2603038:2605074 [0] NCCL INFO  2 : GPU/0 GPU/1\n",
      "node-0:2603038:2605074 [0] NCCL INFO  3 : GPU/0 GPU/1\n",
      "node-0:2603038:2605074 [0] NCCL INFO  4 : GPU/0 GPU/1\n",
      "node-0:2603038:2605074 [0] NCCL INFO  5 : GPU/0 GPU/1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Pattern 1, crossNic 0, nChannels 6, bw 48.000000/48.000000, type XGMI/PIX, sameChannels 0\n",
      "node-0:2603038:2605074 [0] NCCL INFO  0 : GPU/0 GPU/1\n",
      "node-0:2603038:2605074 [0] NCCL INFO  1 : GPU/0 GPU/1\n",
      "node-0:2603038:2605074 [0] NCCL INFO  2 : GPU/0 GPU/1\n",
      "node-0:2603038:2605074 [0] NCCL INFO  3 : GPU/1 GPU/0\n",
      "node-0:2603038:2605074 [0] NCCL INFO  4 : GPU/1 GPU/0\n",
      "node-0:2603038:2605074 [0] NCCL INFO  5 : GPU/1 GPU/0\n",
      "node-0:2603038:2605074 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 6, bw 48.000000/48.000000, type XGMI/PIX, sameChannels 0\n",
      "node-0:2603038:2605074 [0] NCCL INFO  0 : GPU/0 GPU/1\n",
      "node-0:2603038:2605074 [0] NCCL INFO  1 : GPU/0 GPU/1\n",
      "node-0:2603038:2605074 [0] NCCL INFO  2 : GPU/0 GPU/1\n",
      "node-0:2603038:2605074 [0] NCCL INFO  3 : GPU/1 GPU/0\n",
      "node-0:2603038:2605074 [0] NCCL INFO  4 : GPU/1 GPU/0\n",
      "node-0:2603038:2605074 [0] NCCL INFO  5 : GPU/1 GPU/0\n",
      "node-0:2603038:2605075 [1] NCCL INFO Pattern 4, crossNic 0, nChannels 6, bw 24.000000/24.000000, type XGMI/PIX, sameChannels 1\n",
      "node-0:2603038:2605075 [1] NCCL INFO  0 : GPU/0 GPU/1\n",
      "node-0:2603038:2605075 [1] NCCL INFO  1 : GPU/0 GPU/1\n",
      "node-0:2603038:2605075 [1] NCCL INFO  2 : GPU/0 GPU/1\n",
      "node-0:2603038:2605075 [1] NCCL INFO  3 : GPU/0 GPU/1\n",
      "node-0:2603038:2605075 [1] NCCL INFO  4 : GPU/0 GPU/1\n",
      "node-0:2603038:2605075 [1] NCCL INFO  5 : GPU/0 GPU/1\n",
      "node-0:2603038:2605075 [1] NCCL INFO Pattern 1, crossNic 0, nChannels 6, bw 48.000000/48.000000, type XGMI/PIX, sameChannels 0\n",
      "node-0:2603038:2605075 [1] NCCL INFO  0 : GPU/0 GPU/1\n",
      "node-0:2603038:2605075 [1] NCCL INFO  1 : GPU/0 GPU/1\n",
      "node-0:2603038:2605075 [1] NCCL INFO  2 : GPU/0 GPU/1\n",
      "node-0:2603038:2605075 [1] NCCL INFO  3 : GPU/1 GPU/0\n",
      "node-0:2603038:2605075 [1] NCCL INFO  4 : GPU/1 GPU/0\n",
      "node-0:2603038:2605075 [1] NCCL INFO  5 : GPU/1 GPU/0\n",
      "node-0:2603038:2605075 [1] NCCL INFO Pattern 3, crossNic 0, nChannels 6, bw 48.000000/48.000000, type XGMI/PIX, sameChannels 0\n",
      "node-0:2603038:2605075 [1] NCCL INFO  0 : GPU/0 GPU/1\n",
      "node-0:2603038:2605075 [1] NCCL INFO  1 : GPU/0 GPU/1\n",
      "node-0:2603038:2605075 [1] NCCL INFO  2 : GPU/0 GPU/1\n",
      "node-0:2603038:2605075 [1] NCCL INFO  3 : GPU/1 GPU/0\n",
      "node-0:2603038:2605075 [1] NCCL INFO  4 : GPU/1 GPU/0\n",
      "node-0:2603038:2605075 [1] NCCL INFO  5 : GPU/1 GPU/0\n",
      "node-0:2603038:2605075 [1] NCCL INFO Tree 0 : 0 -> 1 -> -1/-1/-1\n",
      "node-0:2603038:2605075 [1] NCCL INFO Tree 6 : 0 -> 1 -> -1/-1/-1\n",
      "node-0:2603038:2605075 [1] NCCL INFO Tree 1 : 0 -> 1 -> -1/-1/-1\n",
      "node-0:2603038:2605075 [1] NCCL INFO Tree 7 : 0 -> 1 -> -1/-1/-1\n",
      "node-0:2603038:2605075 [1] NCCL INFO Tree 2 : 0 -> 1 -> -1/-1/-1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Tree 0 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:2603038:2605075 [1] NCCL INFO Tree 8 : 0 -> 1 -> -1/-1/-1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Tree 6 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:2603038:2605075 [1] NCCL INFO Tree 3 : -1 -> 1 -> 0/-1/-1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Tree 1 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:2603038:2605075 [1] NCCL INFO Tree 9 : -1 -> 1 -> 0/-1/-1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Tree 7 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:2603038:2605075 [1] NCCL INFO Tree 4 : -1 -> 1 -> 0/-1/-1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Tree 2 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:2603038:2605075 [1] NCCL INFO Tree 10 : -1 -> 1 -> 0/-1/-1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Tree 8 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:2603038:2605075 [1] NCCL INFO Tree 5 : -1 -> 1 -> 0/-1/-1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Tree 3 : 1 -> 0 -> -1/-1/-1\n",
      "node-0:2603038:2605075 [1] NCCL INFO Tree 11 : -1 -> 1 -> 0/-1/-1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Tree 9 : 1 -> 0 -> -1/-1/-1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Tree 4 : 1 -> 0 -> -1/-1/-1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Tree 10 : 1 -> 0 -> -1/-1/-1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Tree 5 : 1 -> 0 -> -1/-1/-1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Tree 11 : 1 -> 0 -> -1/-1/-1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 00/24 :    0   1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 01/24 :    0   1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 02/24 :    0   1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 03/24 :    0   1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 04/24 :    0   1\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 0 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 05/24 :    0   1\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 1 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 06/24 :    0   1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 07/24 :    0   1\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 2 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 3 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 4 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 5 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 6 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 7 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 8 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 9 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 08/24 :    0   1\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 10 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 11 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 09/24 :    0   1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 10/24 :    0   1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 11/24 :    0   1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 12/24 :    0   1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 13/24 :    0   1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 14/24 :    0   1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 15/24 :    0   1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 16/24 :    0   1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 17/24 :    0   1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 18/24 :    0   1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 19/24 :    0   1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 20/24 :    0   1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 21/24 :    0   1\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 12 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 22/24 :    0   1\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 23/24 :    0   1\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 13 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 14 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 0 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 15 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 1 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 16 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 2 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 17 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 3 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 18 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 4 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 19 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 5 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 6 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 7 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 8 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 9 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 10 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 20 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 11 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 12 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node-0:2603038:2605075 [1] NCCL INFO Ring 21 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 22 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605075 [1] NCCL INFO Ring 23 : 0 -> 1 -> 0 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605075 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] 0/-1/-1->1->-1 [4] 0/-1/-1->1->-1 [5] 0/-1/-1->1->-1 [6] -1/-1/-1->1->0 [7] -1/-1/-1->1->0 [8] -1/-1/-1->1->0 [9] 0/-1/-1->1->-1 [10] 0/-1/-1->1->-1 [11] 0/-1/-1->1->-1 [12] -1/-1/-1->1->0 [13] -1/-1/-1->1->0 [14] -1/-1/-1->1->0 [15] 0/-1/-1->1->-1 [16] 0/-1/-1->1->-1 [17] 0/-1/-1->1->-1 [18] -1/-1/-1->1->0 [19] -1/-1/-1->1->0 [20] -1/-1/-1->1->0 [21] 0/-1/-1->1->-1 [22] 0/-1/-1->1->-1 [23] 0/-1/-1->1->-1 comm 0xc01cf10 nRanks 02 busId a00000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 13 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 14 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605075 [1] NCCL INFO P2P Chunksize set to 524288\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 15 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 16 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 17 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 18 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 19 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 20 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 21 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 22 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Ring 23 : 1 -> 0 -> 1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605074 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] -1/-1/-1->0->1 [4] -1/-1/-1->0->1 [5] -1/-1/-1->0->1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] -1/-1/-1->0->1 [10] -1/-1/-1->0->1 [11] -1/-1/-1->0->1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] -1/-1/-1->0->1 [16] -1/-1/-1->0->1 [17] -1/-1/-1->0->1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] -1/-1/-1->0->1 [22] -1/-1/-1->0->1 [23] -1/-1/-1->0->1 comm 0xc098360 nRanks 02 busId 900000\n",
      "node-0:2603038:2605074 [0] NCCL INFO P2P Chunksize set to 524288\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 00/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 00/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 01/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 01/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 02/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 02/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 03/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 03/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 04/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 04/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 05/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 05/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 06/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 06/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 07/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 07/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 08/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 08/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 09/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 09/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 10/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 10/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 11/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 11/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 12/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 12/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 13/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 13/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 14/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 14/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 15/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 15/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 16/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 16/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 17/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 17/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 18/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 18/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 19/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 19/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 20/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 20/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 21/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 21/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc098360 nRanks 02\n",
      "node-0:2603038:2605075 [1] NCCL INFO Channel 22/0 : 1[a00000] -> 0[900000] via P2P/direct pointer comm 0xc01cf10 nRanks 02\n",
      "node-0:2603038:2605074 [0] NCCL INFO Channel 22/0 : 0[900000] -> 1[a00000] via P2P/direct pointer comm 0xc09836"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiscuser/scratch/Projects/xcai/xcai/losses.py:21: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:54.)\n",
      "  return torch.sparse_csr_tensor(data_ptr, data_idx, scores, device=data_ptr.device)\n",
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/transformers/utils/import_utils.py:533: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47' max='129900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    47/129900 01:41 < 81:36:32, 0.44 it/s, Epoch 0.11/300]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2603038/1093534927.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/ptca/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1885\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1886\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scratch/Projects/xcai/xcai/learner.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m                 \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m             if (\n",
      "\u001b[0;32m/opt/conda/envs/ptca/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3248\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3252\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/ptca/lib/python3.9/site-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2119\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2120\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2121\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2122\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/ptca/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/envs/ptca/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc60dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cd155e-912d-4780-b60d-b7c6749b30b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == '__main__':\n",
    "    mp.freeze_support()\n",
    "    learn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c09e4-4a84-4e72-8d93-9477b1db3ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
