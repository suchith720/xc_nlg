{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee35a1f",
   "metadata": {},
   "source": [
    "# RADGA Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87faf0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp 37-radga-model-architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c8f9e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a86a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efee8471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os,torch, torch.multiprocessing as mp, pickle\n",
    "from xcai.basics import *\n",
    "from xcai.models.MMM0XX import DBT013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6741403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_MODE'] = 'disabled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b29c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3'\n",
    "os.environ['WANDB_PROJECT']='xc-nlg_37-radga-model-architecture'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b77b08",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a13cac03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiscuser/.local/lib/python3.9/site-packages/xclib-0.97-py3.9-linux-x86_64.egg/xclib/data/data_utils.py:263: UserWarning: Header mis-match from inferred shape!\n",
      "  warnings.warn(\"Header mis-match from inferred shape!\")\n",
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "block = XCBlock.from_cfg('/home/aiscuser/scratch/datasets', 'data_metas', valid_pct=0.001, \n",
    "                         tfm='xcnlg', tokenizer='distilbert-base-uncased', \n",
    "                         smp_features=[('lbl2data',1,2), ('cat2data',1,1), ('hlk2data',1,3)], \n",
    "                         n_data_meta_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2830bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = '/home/aiscuser/scratch/datasets/processed/'\n",
    "with open(f'{pkl_dir}/wikiseealso-radga.pkl', 'wb') as file: pickle.dump(block, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7519f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = '/home/aiscuser/scratch/datasets/processed/'\n",
    "with open(f'{pkl_dir}/wikiseealso-radga.pkl', 'rb') as file: block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ae0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4f41c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiscuser/.local/lib/python3.9/site-packages/xclib-0.97-py3.9-linux-x86_64.egg/xclib/data/data_utils.py:263: UserWarning: Header mis-match from inferred shape!\n",
      "  warnings.warn(\"Header mis-match from inferred shape!\")\n",
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "block = XCBlock.from_cfg('/home/aiscuser/scratch/datasets', 'data_meta', valid_pct=0.001, \n",
    "                         tfm='xcnlg', tokenizer='distilbert-base-uncased', \n",
    "                         smp_features=[('lbl2data',1,2), ('hlk2data',1,3)], n_data_meta_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4d75f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e933404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = XCLearningArguments(\n",
    "    output_dir='/home/aiscuser/outputs/37-radga-model-architecture',\n",
    "    logging_first_step=True,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    representation_num_beams=200,\n",
    "    representation_accumulation_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=5,\n",
    "    num_train_epochs=300,\n",
    "    predict_with_representation=True,\n",
    "    adam_epsilon=1e-6,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.1,\n",
    "    learning_rate=2e-4,\n",
    "    generation_num_beams=10,\n",
    "    generation_length_penalty=1.5,\n",
    "    predict_with_generation=True,\n",
    "    representation_search_type='BRUTEFORCE',\n",
    "    group_by_cluster=True,\n",
    "    num_clustering_warmup_epochs=1,\n",
    "    num_cluster_update_epochs=1,\n",
    "    num_cluster_size_update_epochs=1,\n",
    "    clustering_type='EXPO',\n",
    "    minimum_cluster_size=1,\n",
    "    maximum_cluster_size=300,\n",
    "    output_concatenation_weight=1.0,\n",
    "    metric_for_best_model='P@1_REPR',\n",
    "    target_indices_key='plbl2data_idx',\n",
    "    target_pointer_key='plbl2data_data2ptr',\n",
    "    fp16=True,\n",
    "    label_names=['cat2data_input_ids', 'cat2data_attention_mask', 'cat2data_data2ptr', 'cat2data_idx', \n",
    "                 'pcat2data_idx', 'pcat2data_data2ptr',\n",
    "                 'hlk2data_input_ids', 'hlk2data_attention_mask', 'hlk2data_data2ptr', 'hlk2data_idx', \n",
    "                ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a43e29ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "args = XCLearningArguments(\n",
    "    output_dir='/home/aiscuser/outputs/37-radga-model-architecture',\n",
    "    logging_first_step=True,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    representation_num_beams=200,\n",
    "    representation_accumulation_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=5,\n",
    "    num_train_epochs=300,\n",
    "    predict_with_representation=True,\n",
    "    adam_epsilon=1e-6,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.1,\n",
    "    learning_rate=2e-4,\n",
    "    generation_num_beams=10,\n",
    "    generation_length_penalty=1.5,\n",
    "    predict_with_generation=True,\n",
    "    representation_search_type='BRUTEFORCE',\n",
    "    group_by_cluster=True,\n",
    "    num_clustering_warmup_epochs=1,\n",
    "    num_cluster_update_epochs=1,\n",
    "    num_cluster_size_update_epochs=1,\n",
    "    clustering_type='EXPO',\n",
    "    minimum_cluster_size=1,\n",
    "    maximum_cluster_size=300,\n",
    "    output_concatenation_weight=1.0,\n",
    "    metric_for_best_model='P@1_REPR',\n",
    "    target_indices_key='plbl2data_idx',\n",
    "    target_pointer_key='plbl2data_data2ptr',\n",
    "    fp16=True,\n",
    "    label_names=['hlk2data_input_ids', 'hlk2data_attention_mask', 'hlk2data_data2ptr', 'hlk2data_idx'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7487ae70",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3374cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from xcai.models.MMM0XX import Pooling, XCModelOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fef67bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DBT018(DBT013):\n",
    "    \n",
    "    @delegates(DBT013.__init__)\n",
    "    def __init__(self, config, aug_meta_prefix:Optional[List]=None, tn_meta:Optional[int]=None, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        store_attr('aug_meta_prefix')\n",
    "        self.fuser = Fuser(config)\n",
    "        self.o = torch.ones(tn_meta, dtype=torch.long, device=self.device) if tn_meta is not None else None\n",
    "        \n",
    "    def _get_meta_inputs(self, meta_prefix:Optional[str], **kwargs):\n",
    "        inputs = {}\n",
    "        for t in [o for o in kwargs if meta_prefix is not None and re.match(f'^[p]?{meta_prefix}.*', o)]:\n",
    "            p,q = t.split('_', maxsplit=1)\n",
    "            if t[0] == 'p': inputs.setdefault(p[1:], {})[f'p{q}'] = kwargs[t]\n",
    "            else: inputs.setdefault(p, {})[q] = kwargs[t]\n",
    "        return inputs\n",
    "    \n",
    "    def get_output(\n",
    "        self, \n",
    "        input_ids:Optional[torch.Tensor]=None, \n",
    "        attention_mask:Optional[torch.Tensor]=None, \n",
    "        **kwargs\n",
    "    ):\n",
    "        o = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        return o[0]\n",
    "    \n",
    "    def resize_meta(self, meta:torch.Tensor, mask:torch.Tensor, n_data2meta:torch.Tensor):\n",
    "        bsz, dim, tn_data2meta = n_data2meta.shape[0], meta.shape[-1], meta.shape[0]\n",
    "        self.o = self.o.to(meta.device)\n",
    "        o = (\n",
    "            torch.ones(tn_data2meta, dtype=torch.long, device=meta.device) \n",
    "            if self.o is None or len(self.o) < tn_data2meta else self.o[:tn_data2meta]\n",
    "        )\n",
    "\n",
    "        max_n_data2meta = n_data2meta.max()\n",
    "        xn_data2meta = max_n_data2meta-n_data2meta+1\n",
    "\n",
    "        data2meta_ptr = n_data2meta.cumsum(dim=0)-1\n",
    "        r_data2meta = o.scatter(0, data2meta_ptr, xn_data2meta)\n",
    "\n",
    "        xmeta,xmask = meta.repeat_interleave(r_data2meta, dim=0),mask.repeat_interleave(r_data2meta, dim=0)\n",
    "        m = o.scatter(0, data2meta_ptr, 0).repeat_interleave(r_data2meta, dim=0).view(bsz, -1)\n",
    "        m[:, -1] = 1; m = m.view(-1, 1)\n",
    "        xmask *= m\n",
    "\n",
    "        return xmeta.view(bsz, -1, dim),xmask.view(bsz, -1)\n",
    "    \n",
    "    def get_meta_fused_output(\n",
    "        self, \n",
    "        input_ids:Optional[torch.Tensor]=None, \n",
    "        attention_mask:Optional[torch.Tensor]=None, \n",
    "        **kwargs\n",
    "    ):\n",
    "        data_h,data_mh = self.get_output(input_ids, attention_mask), None\n",
    "        \n",
    "        meta_inputs = self._get_meta_inputs(self.aug_meta_prefix, **kwargs)\n",
    "        for m in meta_inputs.values():\n",
    "            valid_idx = torch.where(m['data2ptr'] > 0)[0]\n",
    "            dmh = data_h.clone()\n",
    "            if len(valid_idx):\n",
    "                meta_h = self.get_output(m['input_ids'], m['attention_mask'])\n",
    "                meta_h, meta_attention_mask = self.resize_meta(meta_h, m['attention_mask'], m['data2ptr'][valid_idx])\n",
    "                dmh[valid_idx] = self.fuser(data_h[valid_idx], meta_h, attention_mask[valid_idx], meta_attention_mask)[0]\n",
    "            data_mh = dmh if data_mh is None else data_mh+dmh\n",
    "            \n",
    "        data_mh = data_h if data_mh is None else data_mh/len(meta_inputs)\n",
    "        return (data_mh,)\n",
    "        \n",
    "    def get_meta_fused_genrep(\n",
    "        self, \n",
    "        input_ids:Optional[torch.Tensor]=None, \n",
    "        attention_mask:Optional[torch.Tensor]=None, \n",
    "        **kwargs\n",
    "    ):    \n",
    "        o = self.get_meta_fused_output(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        rep = self.dr_transform(o[0])\n",
    "        rep = self.dr_layer_norm(rep)\n",
    "        rep = self.dr_projector(rep)\n",
    "        rep = F.normalize(Pooling.mean_pooling(rep, attention_mask), dim=1)\n",
    "        \n",
    "        logits = self.vocab_transform(o[0])\n",
    "        logits = self.activation(logits)\n",
    "        logits = self.vocab_layer_norm(logits)\n",
    "        logits = self.vocab_projector(logits)\n",
    "        return o,logits,rep\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        plbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        plbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):  \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o, data_logits, data_repr = self.get_meta_fused_genrep(data_input_ids, data_attention_mask, **kwargs)\n",
    "        \n",
    "        loss = lm_loss = dr_loss = lbl2data_repr = None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_repr = self.get_representation(lbl2data_input_ids, lbl2data_attention_mask)\n",
    "            lm_loss = self.gen_lfn(data_logits, lbl2data_input_ids, lbl2data_data2ptr, **kwargs)\n",
    "            dr_loss = self.rep_lfn(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, \n",
    "                                   plbl2data_data2ptr, plbl2data_idx, **kwargs)\n",
    "            loss = dr_loss + self.lw*lm_loss\n",
    "            \n",
    "        if not return_dict:\n",
    "            o = (data_logits,data_repr,lbl2data_repr) + data_o[2:]\n",
    "            return ((loss,lm_loss,dr_loss) + o) if loss is not None else o\n",
    "        \n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            lm_loss=lm_loss,\n",
    "            dr_loss=dr_loss,\n",
    "            logits=data_logits,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "            data_hidden_states=data_o[0],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e00117a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DBT019(DBT018):\n",
    "    \n",
    "    @delegates(DBT018.__init__)\n",
    "    def __init__(self, config, m_lw:Optional[float]=0.2, pred_meta_prefix:Optional[List]=None, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        store_attr('m_lw,pred_meta_prefix')\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        plbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        plbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o, data_logits, data_repr = self.get_meta_fused_genrep(data_input_ids, data_attention_mask, **kwargs)\n",
    "        \n",
    "        loss = lm_loss = dr_loss = lbl2data_repr = None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_repr = self.get_representation(lbl2data_input_ids, lbl2data_attention_mask)\n",
    "            lm_loss = self.gen_lfn(data_logits, lbl2data_input_ids, lbl2data_data2ptr, **kwargs)\n",
    "            dr_loss = self.rep_lfn(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, \n",
    "                                   plbl2data_data2ptr, plbl2data_idx, **kwargs)\n",
    "            loss = dr_loss + self.lw*lm_loss\n",
    "            \n",
    "            meta_inputs = self._get_meta_inputs(self.pred_meta_prefix, **kwargs)\n",
    "            m_lw = self.m_lw/len(meta_inputs)\n",
    "            for m in meta_inputs.values():\n",
    "                valid_idx = torch.where(m['data2ptr'])[0]\n",
    "                if len(valid_idx) > 0:\n",
    "                    o, rep = self.get_representation(m['input_ids'], m['attention_mask'])\n",
    "                    m_lml = self.gen_lfn(data_logits[valid_idx], m['input_ids'], m['data2ptr'][valid_idx], **kwargs)\n",
    "                    m_drl = self.rep_lfn(data_repr[valid_idx], rep, m['data2ptr'][valid_idx], m['idx'], \n",
    "                                         m['pdata2ptr'][valid_idx], m['pidx'], **kwargs)\n",
    "                    loss += m_lw * (m_drl + self.lw*m_lml)\n",
    "            \n",
    "        if not return_dict:\n",
    "            o = (data_logits,data_repr,lbl2data_repr) + data_o[2:]\n",
    "            return ((loss,lm_loss,dr_loss) + o) if loss is not None else o\n",
    "        \n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            lm_loss=lm_loss,\n",
    "            dr_loss=dr_loss,\n",
    "            logits=data_logits,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "            data_hidden_states=data_o[0],\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4409b087",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c78706b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset = block.test.dset.sample(n=2000, seed=50)\n",
    "metric = PrecRecl(block.n_lbl, test_dset.data.data_lbl_filterer, prop=block.train.dset.data.data_lbl,\n",
    "                  pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcaa2171",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_575800/1181980701.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbsz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_device_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_device_eval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model = DBT018.from_pretrained('distilbert-base-uncased', ig_tok=0, bsz=bsz, tn_targ=10_000, tn_meta=10_000, \n\u001b[1;32m      4\u001b[0m                                \u001b[0mmargin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_negatives\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_softmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                aug_meta_prefix='hlk', init_drh=True)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()\n",
    "\n",
    "model = DBT018.from_pretrained('distilbert-base-uncased', ig_tok=0, bsz=bsz, tn_targ=10_000, tn_meta=10_000, \n",
    "                               margin=0.3, tau=0.1, n_negatives=5, apply_softmax=True, lw=0.01,\n",
    "                               aug_meta_prefix='hlk', init_drh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e28e8082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT019 were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dr_layer_norm.bias', 'dr_layer_norm.weight', 'dr_projector.bias', 'dr_projector.weight', 'dr_transform.bias', 'dr_transform.weight', 'fuser.k.bias', 'fuser.k.weight', 'fuser.layer_norm.bias', 'fuser.layer_norm.weight', 'fuser.o.bias', 'fuser.o.weight', 'fuser.q.bias', 'fuser.q.weight', 'fuser.v.bias', 'fuser.v.weight', 'gen_lfn.o', 'rep_lfn.u', 'rep_lfn.v']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()\n",
    "\n",
    "model = DBT019.from_pretrained('distilbert-base-uncased', ig_tok=0, bsz=bsz, tn_targ=1000, tn_meta=100, \n",
    "                               margin=0.3, tau=0.1, n_negatives=10, apply_softmax=True, lw=0.01, m_lw=0.1, \n",
    "                               pred_meta_prefix='cat', aug_meta_prefix='hlk', init_drh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "744c0a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63487f702720428696ea147833982d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trie = XCTrie.from_block(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "963533af",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = XCLearner(\n",
    "    model=model, \n",
    "    args=args,\n",
    "    trie=trie,\n",
    "    train_dataset=block.train.dset,\n",
    "    eval_dataset=test_dset,\n",
    "    data_collator=block.collator,\n",
    "    compute_metrics=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b568125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(**b.to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3a90e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1738, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eadd93ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3268334f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
