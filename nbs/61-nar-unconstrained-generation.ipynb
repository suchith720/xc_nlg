{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65a509bb-8dcb-434f-b1a0-023de32ed1c8",
   "metadata": {},
   "source": [
    "# NAR unconstrained generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8df527-ec7d-4884-902d-ce52daded1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp 61-nar-unconstrained-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded2f61-4eb1-44c7-9bb2-ab04df64f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb12a2a-8003-4360-843b-a741772a70c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27154aac-f3f8-42e8-9618-b103069e54aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdc41bb-7d79-403a-9baa-fa53f0caf37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import os, torch, torch.nn.functional as F, pickle\n",
    "from typing import Optional, Dict, Tuple, Union, List, Callable\n",
    "from tqdm.auto import tqdm\n",
    "from xcai.basics import *\n",
    "from xcai.models.MMM0XX import DBT007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d065795-4050-454c-b188-718c99b69b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_MODE'] = 'disabled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2bec93-8178-4af8-8be4-170ecbdab028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['WANDB_PROJECT']='xc-nlg_61-nar-unconstrained-generation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0044e839-cccb-46cc-b22a-7cd2eb31d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'\n",
    "pkl_file = f'{pkl_dir}/processed/wikiseealso_data_distilbert-base-uncased_xcnlg_ngame.pkl'\n",
    "\n",
    "with open(pkl_file, 'rb') as file: block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8014498-02a3-44ca-995b-5a7db8310c6f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2f087-4ca8-41fa-9b42-fb249f0087a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BeamScorer, LogitsProcessorList, StoppingCriteriaList, GenerationConfig\n",
    "from transformers.generation.utils import GenerateOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5664f0f5-7d5c-464c-9aff-83dedc8d679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, inspect, torch.nn as nn\n",
    "from transformers.generation.beam_search import BeamScorer, BeamSearchScorer\n",
    "from transformers.generation.utils import NEED_SETUP_CACHE_CLASSES_MAPPING, GenerationMode\n",
    "from transformers.integrations.deepspeed import is_deepspeed_zero3_enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06bf2e-affe-455f-878d-82fa0cde4f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBT008(DBT007):\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(config, **kwargs)\n",
    "\n",
    "    def can_generate(self): return True\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_kwargs):\n",
    "        return {\"data_input_ids\": input_ids, \"data_attention_mask\": model_kwargs['data_attention_mask']}\n",
    "\n",
    "    def _prepare_model_inputs(\n",
    "        self,\n",
    "        inputs: Optional[torch.Tensor] = None,\n",
    "        bos_token_id: Optional[torch.Tensor] = None,\n",
    "        model_kwargs: Optional[Dict[str, torch.Tensor]] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[str], Dict[str, torch.Tensor]]:\n",
    "        model_kwargs = {k: v for k, v in model_kwargs.items() if v is not None or k != 'data_input_ids'}\n",
    "\n",
    "        inputs_kwarg = model_kwargs.pop('data_input_ids', None)\n",
    "        if inputs_kwarg is not None and inputs is not None:\n",
    "            raise ValueError(\n",
    "                f\"`inputs`: {inputs}` were passed alongside {input_name} which is not allowed. \"\n",
    "                f\"Make sure to either pass {inputs} or {input_name}=...\"\n",
    "            )\n",
    "        elif inputs_kwarg is not None:\n",
    "            inputs = inputs_kwarg\n",
    "            \n",
    "        return inputs, 'data_input_ids', model_kwargs\n",
    "        \n",
    "    def group_beam_search(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        beam_scorer: BeamScorer,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        bos_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        output_logits: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "        # init values\n",
    "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        if max_length is not None:\n",
    "            warnings.warn(\n",
    "                \"`max_length` is deprecated in this function, use\"\n",
    "                \" `stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])` instead.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            stopping_criteria = validate_stopping_criteria(stopping_criteria, max_length)\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "        bos_token_id = bos_token_id if bos_token_id is not None else self.generation_config.bos_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int): eos_token_id = [eos_token_id]\n",
    "        output_scores = output_scores if output_scores is not None else self.generation_config.output_scores\n",
    "        output_logits = output_logits if output_logits is not None else self.generation_config.output_logits\n",
    "        output_attentions = (\n",
    "            output_attentions if output_attentions is not None else self.generation_config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.generation_config.output_hidden_states\n",
    "        )\n",
    "        return_dict_in_generate = (\n",
    "            return_dict_in_generate\n",
    "            if return_dict_in_generate is not None\n",
    "            else self.generation_config.return_dict_in_generate\n",
    "        )\n",
    "\n",
    "        num_beams = beam_scorer.num_beams\n",
    "        num_beam_groups = beam_scorer.num_beam_groups\n",
    "        num_sub_beams = num_beams // num_beam_groups\n",
    "        batch_size = len(beam_scorer._beam_hyps) // num_beam_groups\n",
    "        device = input_ids.device\n",
    "\n",
    "        batch_beam_size, cur_len = num_beams * input_ids.shape[0], 1\n",
    "\n",
    "        if return_dict_in_generate and output_scores:\n",
    "            beam_indices = [tuple(() for _ in range(num_sub_beams * batch_size)) for _ in range(num_beam_groups)]\n",
    "        else:\n",
    "            beam_indices = None\n",
    "\n",
    "        if num_beams * batch_size != batch_beam_size:\n",
    "            raise ValueError(\n",
    "                f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n",
    "            )\n",
    "\n",
    "        # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        raw_logits = () if (return_dict_in_generate and output_logits) else None\n",
    "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "        if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "            encoder_hidden_states = (\n",
    "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "            )\n",
    "        # initialise score of first beam of each group with 0 and the rest with -1e9. This ensures that the beams in\n",
    "        # the same group don't produce same tokens everytime.\n",
    "        beam_scores = torch.full((batch_size, num_beams), -1e9, dtype=torch.float, device=device)\n",
    "        beam_scores[:, ::num_sub_beams] = 0\n",
    "        beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "\n",
    "        this_peer_finished = False  # used by synced_gpus only\n",
    "\n",
    "        \n",
    "        # do all decoder steps on all beams of all sentences in batch\n",
    "        model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "        outputs = self(\n",
    "            **model_inputs,\n",
    "            return_dict=True,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "        output_ids = torch.full((batch_beam_size,1), bos_token_id, dtype=input_ids.dtype, device=input_ids.device)\n",
    "        \n",
    "        while True:\n",
    "            if synced_gpus:\n",
    "                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n",
    "                # The following logic allows an early break if all peers finished generating their sequence\n",
    "                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n",
    "                # send 0.0 if we finished, 1.0 otherwise\n",
    "                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n",
    "                # did all peers finish? the reduced sum will be 0.0 then\n",
    "                if this_peer_finished_flag.item() == 0.0:\n",
    "                    break\n",
    "\n",
    "            # predicted tokens in cur_len step\n",
    "            current_tokens = torch.zeros(batch_size * num_beams, dtype=input_ids.dtype, device=device)\n",
    "\n",
    "            # indices which will form the beams in the next time step\n",
    "            reordering_indices = torch.zeros(batch_size * num_beams, dtype=torch.long, device=device)\n",
    "\n",
    "            if synced_gpus and this_peer_finished:\n",
    "                cur_len = cur_len + 1\n",
    "                continue  # don't waste resources running the code we don't need\n",
    "\n",
    "            if output_scores:\n",
    "                processed_score = torch.zeros_like(outputs.logits[:, cur_len, :])\n",
    "            if output_logits:\n",
    "                raw_logit_score = outputs.logits[:, cur_len, :]\n",
    "            \n",
    "            for beam_group_idx in range(num_beam_groups):\n",
    "                group_start_idx = beam_group_idx * num_sub_beams\n",
    "                group_end_idx = min(group_start_idx + num_sub_beams, num_beams)\n",
    "                group_size = group_end_idx - group_start_idx\n",
    "\n",
    "                # indices of beams of current group among all sentences in batch\n",
    "                batch_group_indices = []\n",
    "                for batch_idx in range(batch_size):\n",
    "                    batch_group_indices.extend(\n",
    "                        [batch_idx * num_beams + idx for idx in range(group_start_idx, group_end_idx)]\n",
    "                    )\n",
    "                group_input_ids = output_ids[batch_group_indices]\n",
    "                \n",
    "                next_token_logits = outputs.logits[:, cur_len:cur_len+1]\n",
    "                next_token_scores = nn.functional.log_softmax(\n",
    "                    next_token_logits, dim=-1\n",
    "                )  # (batch_size * group_size, vocab_size)\n",
    "                vocab_size = next_token_scores.shape[-1]\n",
    "                \n",
    "                next_token_scores = next_token_scores.expand(-1, group_size, -1).contiguous().view(batch_size * group_size, -1)\n",
    "\n",
    "                next_token_scores_processed = logits_processor(\n",
    "                    group_input_ids, next_token_scores, current_tokens=current_tokens, beam_group_idx=beam_group_idx\n",
    "                )\n",
    "                next_token_scores = next_token_scores_processed + beam_scores[batch_group_indices].unsqueeze(-1)\n",
    "                next_token_scores = next_token_scores.expand_as(next_token_scores_processed)\n",
    "\n",
    "                if output_scores:\n",
    "                    processed_score[batch_group_indices] = next_token_scores_processed\n",
    "\n",
    "                next_token_scores = next_token_scores.view(batch_size, group_size * vocab_size)\n",
    "\n",
    "                # Sample 1 + len(eos_token_id) next tokens for each beam so we have at least 1 non eos token per beam.\n",
    "                n_eos_tokens = len(eos_token_id) if eos_token_id else 0\n",
    "                next_token_scores, next_tokens = torch.topk(\n",
    "                    next_token_scores, max(2, 1 + n_eos_tokens) * group_size, dim=1, largest=True, sorted=True\n",
    "                )\n",
    "\n",
    "                next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n",
    "                next_tokens = next_tokens % vocab_size\n",
    "\n",
    "                # stateless\n",
    "                process_beam_indices = sum(beam_indices, ()) if beam_indices is not None else None\n",
    "                beam_outputs = beam_scorer.process(\n",
    "                    group_input_ids,\n",
    "                    next_token_scores,\n",
    "                    next_tokens,\n",
    "                    next_indices,\n",
    "                    pad_token_id=pad_token_id,\n",
    "                    eos_token_id=eos_token_id,\n",
    "                    beam_indices=process_beam_indices,\n",
    "                    group_index=beam_group_idx,\n",
    "                    decoder_prompt_len=0,\n",
    "                )\n",
    "                beam_scores[batch_group_indices] = beam_outputs[\"next_beam_scores\"]\n",
    "                beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "                beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "\n",
    "                if return_dict_in_generate and output_scores:\n",
    "                    beam_indices[beam_group_idx] = tuple(\n",
    "                        beam_indices[beam_group_idx][beam_idx[i]] + (beam_idx[i],) for i in range(len(beam_indices[0]))\n",
    "                    )\n",
    "\n",
    "                output_ids[batch_group_indices] = group_input_ids[beam_idx]\n",
    "                group_input_ids = torch.cat([group_input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "                current_tokens[batch_group_indices] = group_input_ids[:, -1]\n",
    "\n",
    "                # (beam_idx // group_size) -> batch_idx\n",
    "                # (beam_idx % group_size) -> offset of idx inside the group\n",
    "                reordering_indices[batch_group_indices] = (\n",
    "                    num_beams * torch.div(beam_idx, group_size, rounding_mode=\"floor\")\n",
    "                    + group_start_idx\n",
    "                    + (beam_idx % group_size)\n",
    "                )\n",
    "            \n",
    "            # Store scores, attentions and hidden_states when required\n",
    "            if return_dict_in_generate:\n",
    "                if output_scores:\n",
    "                    scores += (processed_score,)\n",
    "                if output_logits:\n",
    "                    raw_logits += (raw_logit_score,)\n",
    "                if output_attentions:\n",
    "                    decoder_attentions += (\n",
    "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                    )\n",
    "                    if self.config.is_encoder_decoder:\n",
    "                        cross_attentions += (outputs.cross_attentions,)\n",
    "\n",
    "                if output_hidden_states:\n",
    "                    decoder_hidden_states += (\n",
    "                        (outputs.decoder_hidden_states,)\n",
    "                        if self.config.is_encoder_decoder\n",
    "                        else (outputs.hidden_states,)\n",
    "                    )\n",
    "\n",
    "            output_ids = torch.cat([output_ids, current_tokens.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(\n",
    "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
    "            )\n",
    "            if model_kwargs[\"past_key_values\"] is not None:\n",
    "                model_kwargs[\"past_key_values\"] = self._temporary_reorder_cache(\n",
    "                    model_kwargs[\"past_key_values\"], reordering_indices\n",
    "                )\n",
    "\n",
    "            # increase cur_len\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "            if beam_scorer.is_done or stopping_criteria(output_ids, scores):\n",
    "                if not synced_gpus:\n",
    "                    break\n",
    "                else:\n",
    "                    this_peer_finished = True\n",
    "                    \n",
    "        final_beam_indices = sum(beam_indices, ()) if beam_indices is not None else None\n",
    "        sequence_outputs = beam_scorer.finalize(\n",
    "            output_ids,\n",
    "            beam_scores,\n",
    "            next_tokens,\n",
    "            next_indices,\n",
    "            pad_token_id=pad_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            max_length=stopping_criteria.max_length,\n",
    "            beam_indices=final_beam_indices,\n",
    "            decoder_prompt_len=0,\n",
    "        )\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            if not output_scores:\n",
    "                sequence_outputs[\"sequence_scores\"] = None\n",
    "\n",
    "            if self.config.is_encoder_decoder:\n",
    "                return GenerateBeamEncoderDecoderOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    logits=raw_logits,\n",
    "                    beam_indices=sequence_outputs[\"beam_indices\"],\n",
    "                    encoder_attentions=encoder_attentions,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    decoder_attentions=decoder_attentions,\n",
    "                    cross_attentions=cross_attentions,\n",
    "                    decoder_hidden_states=decoder_hidden_states,\n",
    "                    past_key_values=model_kwargs.get(\"past_key_values\"),\n",
    "                )\n",
    "            else:\n",
    "                return GenerateBeamDecoderOnlyOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    logits=raw_logits,\n",
    "                    beam_indices=sequence_outputs[\"beam_indices\"],\n",
    "                    attentions=decoder_attentions,\n",
    "                    hidden_states=decoder_hidden_states,\n",
    "                    past_key_values=model_kwargs.get(\"past_key_values\"),\n",
    "                )\n",
    "        else:\n",
    "            return sequence_outputs[\"sequences\"]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        inputs: Optional[torch.Tensor] = None,\n",
    "        generation_config: Optional[GenerationConfig] = None,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
    "        synced_gpus: Optional[bool] = None,\n",
    "        assistant_model: Optional[\"PreTrainedModel\"] = None,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        negative_prompt_ids: Optional[torch.Tensor] = None,\n",
    "        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Union[GenerateOutput, torch.LongTensor]:\n",
    "        if synced_gpus is None:\n",
    "            if is_deepspeed_zero3_enabled() and dist.get_world_size() > 1:\n",
    "                synced_gpus = True\n",
    "            else:\n",
    "                synced_gpus = False\n",
    "\n",
    "        # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\n",
    "        self._validate_model_class()\n",
    "\n",
    "        # priority: `generation_config` argument > `model.generation_config` (the default generation config)\n",
    "        if generation_config is None:\n",
    "            # legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,\n",
    "            # three conditions must be met\n",
    "            # 1) the generation config must have been created from the model config (`_from_model_config` field);\n",
    "            # 2) the generation config must have seen no modification since its creation (the hash is the same);\n",
    "            # 3) the user must have set generation parameters in the model config.\n",
    "            if (\n",
    "                self.generation_config._from_model_config\n",
    "                and self.generation_config._original_object_hash == hash(self.generation_config)\n",
    "                and self.config._has_non_default_generation_parameters()\n",
    "            ):\n",
    "                new_generation_config = GenerationConfig.from_model_config(self.config)\n",
    "                if new_generation_config != self.generation_config:\n",
    "                    warnings.warn(\n",
    "                        \"You have modified the pretrained model configuration to control generation. This is a\"\n",
    "                        \" deprecated strategy to control generation and will be removed soon, in a future version.\"\n",
    "                        \" Please use and modify the model generation configuration (see\"\n",
    "                        \" https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\"\n",
    "                    )\n",
    "                    self.generation_config = new_generation_config\n",
    "            generation_config = self.generation_config\n",
    "\n",
    "        generation_config = copy.deepcopy(generation_config)\n",
    "        model_kwargs = generation_config.update(**kwargs)  # All unused kwargs must be model kwargs\n",
    "        self._validate_model_kwargs(model_kwargs.copy())\n",
    "\n",
    "        # 2. Set generation parameters if not already defined\n",
    "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "\n",
    "        if generation_config.pad_token_id is None and generation_config.eos_token_id is not None:\n",
    "            if model_kwargs.get(\"attention_mask\", None) is None:\n",
    "                logger.warning(\n",
    "                    \"The attention mask and the pad token id were not set. As a consequence, you may observe \"\n",
    "                    \"unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\n",
    "                )\n",
    "            eos_token_id = generation_config.eos_token_id\n",
    "            if isinstance(eos_token_id, list):\n",
    "                eos_token_id = eos_token_id[0]\n",
    "            logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n",
    "            generation_config.pad_token_id = eos_token_id\n",
    "\n",
    "        # 3. Define model inputs\n",
    "        # inputs_tensor has to be defined\n",
    "        # model_input_name is defined if model-specific keyword input is passed\n",
    "        # otherwise model_input_name is None\n",
    "        # all model-specific keyword inputs are removed from `model_kwargs`\n",
    "        inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(\n",
    "            inputs, generation_config.bos_token_id, model_kwargs\n",
    "        )\n",
    "        batch_size = inputs_tensor.shape[0]\n",
    "\n",
    "        # 4. Define other model kwargs\n",
    "        model_kwargs[\"output_attentions\"] = generation_config.output_attentions\n",
    "        model_kwargs[\"output_hidden_states\"] = generation_config.output_hidden_states\n",
    "        # decoder-only models with inputs_embeds forwarding must use caching (otherwise we can't detect whether we are\n",
    "        # generating the first new token or not, and we only want to use the embeddings for the first new token)\n",
    "        if model_input_name == \"inputs_embeds\": model_kwargs[\"use_cache\"] = True\n",
    "        else: model_kwargs[\"use_cache\"] = generation_config.use_cache\n",
    "\n",
    "        accepts_attention_mask = \"attention_mask\" in set(inspect.signature(self.forward).parameters.keys())\n",
    "        requires_attention_mask = \"encoder_outputs\" not in model_kwargs\n",
    "\n",
    "        if model_kwargs.get(\"attention_mask\", None) is None and requires_attention_mask and accepts_attention_mask:\n",
    "            model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n",
    "                inputs_tensor, generation_config.pad_token_id, generation_config.eos_token_id\n",
    "            )\n",
    "\n",
    "\n",
    "        # 5. Prepare `input_ids` which will be used for auto-regressive generation\n",
    "        input_ids = inputs_tensor if model_input_name == \"data_input_ids\" else model_kwargs.pop(\"input_ids\")\n",
    "        if streamer is not None: streamer.put(input_ids.cpu())\n",
    "\n",
    "        # 6. Prepare `max_length` depending on other stopping criteria.\n",
    "        input_ids_length = input_ids.shape[-1]\n",
    "        generation_config.max_length = min(input_ids_length, generation_config.max_length)\n",
    "\n",
    "        # 7. determine generation mode\n",
    "        generation_mode = self._get_generation_mode(generation_config, assistant_model)\n",
    "\n",
    "        if streamer is not None and (generation_config.num_beams > 1):\n",
    "            raise ValueError(\n",
    "                \"`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.\"\n",
    "            )\n",
    "\n",
    "        if self.device.type != input_ids.device.type:\n",
    "            warnings.warn(\n",
    "                \"You are calling .generate() with the `input_ids` being on a device type different\"\n",
    "                f\" than your model's device. `input_ids` is on {input_ids.device.type}, whereas the model\"\n",
    "                f\" is on {self.device.type}. You may experience unexpected behaviors or slower generation.\"\n",
    "                \" Please make sure that you have put `input_ids` to the\"\n",
    "                f\" correct device by calling for example input_ids = input_ids.to('{self.device.type}') before\"\n",
    "                \" running `.generate()`.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "\n",
    "        # 8. prepare distribution pre_processing samplers\n",
    "        prepared_logits_processor = self._get_logits_processor(\n",
    "            generation_config=generation_config,\n",
    "            input_ids_seq_length=input_ids_length,\n",
    "            encoder_input_ids=inputs_tensor,\n",
    "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "            logits_processor=logits_processor,\n",
    "            model_kwargs=model_kwargs,\n",
    "            negative_prompt_ids=negative_prompt_ids,\n",
    "            negative_prompt_attention_mask=negative_prompt_attention_mask,\n",
    "        )\n",
    "\n",
    "        # 9. prepare stopping criteria\n",
    "        prepared_stopping_criteria = self._get_stopping_criteria(\n",
    "            generation_config=generation_config, stopping_criteria=stopping_criteria\n",
    "        )\n",
    "\n",
    "        if generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n",
    "            # 10. prepare beam search scorer\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=generation_config.num_beams,\n",
    "                device=inputs_tensor.device,\n",
    "                length_penalty=generation_config.length_penalty,\n",
    "                do_early_stopping=generation_config.early_stopping,\n",
    "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
    "                num_beam_groups=generation_config.num_beam_groups,\n",
    "                max_length=generation_config.max_length,\n",
    "            )\n",
    "            \n",
    "            # 11. run beam search\n",
    "            return self.group_beam_search(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                pad_token_id=generation_config.pad_token_id,\n",
    "                bos_token_id=generation_config.bos_token_id,\n",
    "                eos_token_id=generation_config.eos_token_id,\n",
    "                output_scores=generation_config.output_scores,\n",
    "                output_logits=generation_config.output_logits,\n",
    "                return_dict_in_generate=generation_config.return_dict_in_generate,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "        else: \n",
    "            raise NotImplementedError('Only Diverse beam search is supported.')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c506a1-3dd4-43cd-8f16-2f2145b4449e",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e8e8f8-02f8-48a1-8b98-62709fb605ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken, math, pickle\n",
    "from stop_words import get_stop_words\n",
    "from langdetect import detect\n",
    "from typing import List\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a4249-c8fd-40f1-a84e-a7b1033ce564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multilingual_stopwords(text: str) -> str:\n",
    "    # Detect the language of the text\n",
    "    try: lang = detect(text)\n",
    "    except: return text\n",
    "\n",
    "    # Get the list of stop words for the detected language\n",
    "    try: stop_words = set(get_stop_words(lang))\n",
    "    except: return text\n",
    "\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "encoder = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "def preprocess_func(text: str) -> List[str]:\n",
    "    lowered = text.lower()\n",
    "    tokens = encoder.encode(lowered)\n",
    "    return [str(token) for token in tokens]\n",
    "\n",
    "def tokenize(text): return preprocess_func(remove_multilingual_stopwords(text))\n",
    "\n",
    "def tokenizer(text): \n",
    "    return [tokenize(o) for o in tqdm(text, total=len(text))]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9379976-e190-418c-a126-f6ddd3b8560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/scai/phd/aiz218323/scratch/outputs/60-nar-inference-pipeline-for-distilbert'\n",
    "with open(f'{data_dir}/wikiseealso-lbl.bow', 'rb') as file: \n",
    "    lbl_bow = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175a054-e04c-4a72-b648-1bca3eb1580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(lbl_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05700f54-4acd-499b-9e98-804b62b6550c",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f38c12e-8748-4941-b048-9df861380dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "mdir = '/home/scai/phd/aiz218323/scratch/outputs/20-nar-training-pipeline-for-distilbert-2-1'\n",
    "mname = get_best_model(mdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e18b69-e7a0-442b-8bca-046b40e39b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DBT008.from_pretrained(mname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3773bf3f-dfe2-4e08-8428-f6b46a96e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = XCLearningArguments(\n",
    "    output_dir='/home/scai/phd/aiz218323/scratch/outputs/20-nar-training-pipeline-for-distilbert-2-1',\n",
    "    logging_first_step=True,\n",
    "    per_device_train_batch_size=1024,\n",
    "    per_device_eval_batch_size=64,\n",
    "    representation_num_beams=200,\n",
    "    representation_accumulation_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy='steps',\n",
    "    generation_num_beams=10,\n",
    "    generation_length_penalty=1.5,\n",
    "    predict_with_generation=True,\n",
    "    target_indices_key='plbl2data_idx',\n",
    "    target_pointer_key='plbl2data_data2ptr',\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10311fb-0184-488b-9c3d-624f9c1089ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset = block.test.dset.sample(n=2000, seed=50)\n",
    "metric = PrecRecl(block.n_lbl, test_dset.data.data_lbl_filterer, prop=block.train.dset.data.data_lbl,\n",
    "                  pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f2b147-a2f3-4d59-adf3-1110e1774b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "learn = XCLearner(\n",
    "    model=model, \n",
    "    args=args,\n",
    "    train_dataset=block.train.dset,\n",
    "    eval_dataset=test_dset,\n",
    "    data_collator=block.collator,\n",
    "    compute_metrics=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb78dc5-bff9-445f-9c93-45cbfde43715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokz = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f844fb-a042-4987-95d0-642b229691c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939f12a7e7f94214bc414f604627d1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output,rewrite,num_return_sequences = [],[],10\n",
    "for b in tqdm(dl):\n",
    "    b = b.to(model.device)\n",
    "    o = model.generate(**b, num_beams=20, num_beam_groups=2, diversity_penalty=0.8, length_penalty=1.5, \n",
    "                       bos_token_id=101, eos_token_id=102, num_return_sequences=num_return_sequences)\n",
    "    toks = tokz.batch_decode(o, skip_special_tokens=True)\n",
    "    \n",
    "    for i in range(0, len(toks), num_return_sequences):\n",
    "        rewrite.append(\" \".join(toks[i:i+num_return_sequences]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8104929-7c27-49dd-9bf8-38c9ddcbfc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204515274d964a4295886dd29f91679b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_bow = tokenizer(rewrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37855f0c-b7cb-452d-8e46-3923d1544524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a050aadd1048429e2be5d78b6b72e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_bow = tokenizer(test_dset.data.data_info['input_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103fe24b-ddc7-4183-8ea1-1c2233e0a363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf2a223f22749eebf9b599283739641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_score, pred_idx, pred_ptr = [], [], []\n",
    "for o in tqdm(test_bow):\n",
    "    sc = bm25.get_scores(o)\n",
    "    score,idx = torch.topk(torch.tensor(sc), 200)\n",
    "    pred_score.extend(score)\n",
    "    pred_idx.extend(idx)\n",
    "    pred_ptr.append(200)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee0590-8b77-49f7-98bb-c442965d1dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_idx = test_dset.data.data_lbl.indices\n",
    "targ_ptr = [o.getnnz() for o in test_dset.data.data_lbl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc07000-8551-4118-bd15-3ba4200a6ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'pred_score': torch.tensor(pred_score),\n",
    "    'pred_idx': torch.tensor(pred_idx),\n",
    "    'pred_ptr': torch.tensor(pred_ptr),\n",
    "    'targ_idx': torch.tensor(targ_idx),\n",
    "    'targ_ptr': torch.tensor(targ_ptr),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62c2ea-605d-44cc-af0c-e310977e2134",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/scai/phd/aiz218323/scratch/outputs/60-nar-inference-pipeline-for-distilbert'\n",
    "with open(f'{data_dir}/wikiseealso-output.pkl', 'wb') as file: \n",
    "    pickle.dump(output, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262f065-68c1-43e7-8510-e6cfb890e95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/scipy/sparse/_index.py:145: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "m = metric(**output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021a3cd-365b-40a1-9601-ce0f7e86f51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>P@5</th>\n",
       "      <th>P@10</th>\n",
       "      <th>N@1</th>\n",
       "      <th>N@3</th>\n",
       "      <th>N@5</th>\n",
       "      <th>N@10</th>\n",
       "      <th>PSP@1</th>\n",
       "      <th>PSP@3</th>\n",
       "      <th>PSP@5</th>\n",
       "      <th>PSP@10</th>\n",
       "      <th>PSN@1</th>\n",
       "      <th>PSN@3</th>\n",
       "      <th>PSN@5</th>\n",
       "      <th>PSN@10</th>\n",
       "      <th>R@10</th>\n",
       "      <th>R@100</th>\n",
       "      <th>R@200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.1</td>\n",
       "      <td>6.45</td>\n",
       "      <td>4.75</td>\n",
       "      <td>2.885</td>\n",
       "      <td>12.1</td>\n",
       "      <td>10.7808</td>\n",
       "      <td>11.0679</td>\n",
       "      <td>11.5963</td>\n",
       "      <td>8.9719</td>\n",
       "      <td>7.5856</td>\n",
       "      <td>8.0409</td>\n",
       "      <td>9.0389</td>\n",
       "      <td>8.9719</td>\n",
       "      <td>8.5993</td>\n",
       "      <td>9.0692</td>\n",
       "      <td>9.7045</td>\n",
       "      <td>12.9182</td>\n",
       "      <td>22.7151</td>\n",
       "      <td>26.5959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    P@1   P@3   P@5   P@10   N@1      N@3      N@5     N@10   PSP@1   PSP@3  \\\n",
       "0  12.1  6.45  4.75  2.885  12.1  10.7808  11.0679  11.5963  8.9719  7.5856   \n",
       "\n",
       "    PSP@5  PSP@10   PSN@1   PSN@3   PSN@5  PSN@10     R@10    R@100    R@200  \n",
       "0  8.0409  9.0389  8.9719  8.5993  9.0692  9.7045  12.9182  22.7151  26.5959  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_metric(m, remove_prefix=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092926ba-cc68-4132-b957-2aa3559de7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>P@5</th>\n",
       "      <th>P@10</th>\n",
       "      <th>N@1</th>\n",
       "      <th>N@3</th>\n",
       "      <th>N@5</th>\n",
       "      <th>N@10</th>\n",
       "      <th>PSP@1</th>\n",
       "      <th>PSP@3</th>\n",
       "      <th>PSP@5</th>\n",
       "      <th>PSP@10</th>\n",
       "      <th>PSN@1</th>\n",
       "      <th>PSN@3</th>\n",
       "      <th>PSN@5</th>\n",
       "      <th>PSN@10</th>\n",
       "      <th>R@10</th>\n",
       "      <th>R@100</th>\n",
       "      <th>R@200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.05</td>\n",
       "      <td>7.6167</td>\n",
       "      <td>5.68</td>\n",
       "      <td>3.515</td>\n",
       "      <td>13.05</td>\n",
       "      <td>11.6593</td>\n",
       "      <td>12.0307</td>\n",
       "      <td>12.7358</td>\n",
       "      <td>12.3423</td>\n",
       "      <td>11.2823</td>\n",
       "      <td>11.7735</td>\n",
       "      <td>13.1454</td>\n",
       "      <td>12.3423</td>\n",
       "      <td>11.9975</td>\n",
       "      <td>12.5725</td>\n",
       "      <td>13.4752</td>\n",
       "      <td>14.8224</td>\n",
       "      <td>24.6617</td>\n",
       "      <td>27.6057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     P@1     P@3   P@5   P@10    N@1      N@3      N@5     N@10    PSP@1  \\\n",
       "0  13.05  7.6167  5.68  3.515  13.05  11.6593  12.0307  12.7358  12.3423   \n",
       "\n",
       "     PSP@3    PSP@5   PSP@10    PSN@1    PSN@3    PSN@5   PSN@10     R@10  \\\n",
       "0  11.2823  11.7735  13.1454  12.3423  11.9975  12.5725  13.4752  14.8224   \n",
       "\n",
       "     R@100    R@200  \n",
       "0  24.6617  27.6057  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_metric(m, remove_prefix=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9698b-8708-4242-b60c-7b94fc370ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
