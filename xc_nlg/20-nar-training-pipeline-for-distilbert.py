# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/20-nar-training-pipeline-for-distilbert-Copy1.ipynb.

# %% auto 0
__all__ = ['block', 'lbl_tok_tfidf', 'valid_toks', 'input_ids', 'mask', 'tok_idf', 'args', 'test_dset', 'metric', 'model', 'trie',
           'learn', 'compute_tok_tfidf', 'sparse_threshold_mask', 'prune_tokens']

# %% ../nbs/20-nar-training-pipeline-for-distilbert-Copy1.ipynb 4
import os, torch, torch.nn.functional as F, pickle
from tqdm.auto import tqdm
from xcai.basics import *
from xcai.models.MMM00X import DBT007
from xcai.transform import TriePruneInputIdsTfm

# %% ../nbs/20-nar-training-pipeline-for-distilbert-Copy1.ipynb 7
import numpy as np
from typing import Optional, List
from scipy import sparse

def compute_tok_tfidf(toks:Optional[List], n_cols:Optional[int]=None):
    tok_sparse = get_tok_sparse(toks, n_cols=n_cols)
    tok_idf = torch.tensor(tok_sparse.getnnz(axis=0))
    tok_idf = torch.log((tok_sparse.shape[0]+1)/(tok_idf+1)) + 1
    return tok_sparse.multiply(tok_idf.numpy())

def sparse_threshold_mask(m, min_val:int, max_val:Optional[int]=None):
    v_m = m > min_val
    if max_val is not None:
        r,c = (m > max_val).nonzero()
        v_m[r,c] = 0
        v_m.eliminate_zeros()
    return v_m

def prune_tokens(input_ids, attention_mask, valid_toks):
    p_input_ids,p_attention_mask = [],[]
    for i,(ids,mask) in tqdm(enumerate(zip(input_ids, attention_mask)), total=len(input_ids)):
        p_input_ids.append([t for t in ids if valid_toks[i,t]])
        p_attention_mask.append(mask[:len(p_input_ids[-1])])
    return p_input_ids,p_attention_mask


# %% ../nbs/20-nar-training-pipeline-for-distilbert-Copy1.ipynb 11
os.environ['CUDA_VISIBLE_DEVICES'] = '15'
os.environ['WANDB_PROJECT']='xc-nlg_20-nar-training-pipeline-for-distilbert'

# %% ../nbs/20-nar-training-pipeline-for-distilbert-Copy1.ipynb 12
block = XCBlock.from_cfg('/home/aiscuser/scratch/datasets', 'data', dset='amazontitles', valid_pct=0.001, 
                         tfm='xcnlg', tokenizer='distilbert-base-uncased', smp_features=[('lbl2data',1,2)])

# %% ../nbs/20-nar-training-pipeline-for-distilbert-Copy1.ipynb 16
lbl_tok_tfidf = compute_tok_tfidf(block.train.dset.data.lbl_info['input_ids'], n_cols=30522)

# %% ../nbs/20-nar-training-pipeline-for-distilbert-Copy1.ipynb 18
valid_toks = sparse_threshold_mask(lbl_tok_tfidf, min_val=7)
valid_toks[:, 101] = valid_toks[:, 102] = 1

# %% ../nbs/20-nar-training-pipeline-for-distilbert-Copy1.ipynb 20
input_ids,mask = prune_tokens(block.train.dset.lbl_info['input_ids'],block.train.dset.lbl_info['attention_mask'],valid_toks)
block.train.dset.lbl_info['input_ids'], block.train.dset.lbl_info['attention_mask'] = input_ids, mask
block.test.dset.lbl_info['input_ids'], block.test.dset.lbl_info['attention_mask'] = input_ids, mask

# %% ../nbs/20-nar-training-pipeline-for-distilbert-Copy1.ipynb 22
block = TriePruneInputIdsTfm.apply(block, 'train.dset.data.lbl_info')

# %% ../nbs/20-nar-training-pipeline-for-distilbert-Copy1.ipynb 24
tok_idf = get_tok_idf(block.train.dset, field='lbl2data_input_ids', n_cols=30522)

# %% ../nbs/20-nar-training-pipeline-for-distilbert-Copy1.ipynb 27
args = XCLearningArguments(
    output_dir='/home/aiscuser/outputs/20-nar-training-pipeline-for-distilbert-2-6',
    logging_first_step=True,
    per_device_train_batch_size=1024,
    per_device_eval_batch_size=1024,
    representation_num_beams=200,
    representation_accumulation_steps=100,
    save_strategy="steps",
    evaluation_strategy='steps',
    eval_steps=2000,
    save_steps=2000,
    save_total_limit=5,
    num_train_epochs=100,
    adam_epsilon=1e-8,
    warmup_steps=0,
    weight_decay=0.1,
    learning_rate=2e-4,
    generation_num_beams=10,
    generation_length_penalty=1.5,
    predict_with_generation=True,
    label_names=['lbl2data_idx'],
)

# %% ../nbs/20-nar-training-pipeline-for-distilbert-Copy1.ipynb 28
test_dset = block.test.dset.sample(n=2000, seed=50)
metric = PrecRecl(block.n_lbl, test_dset.data.data_lbl_filterer, prop=block.train.dset.data.data_lbl,
                  pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])

# %% ../nbs/20-nar-training-pipeline-for-distilbert-Copy1.ipynb 29
model = DBT007.from_pretrained('distilbert-base-uncased', tn_targ=10_000, ig_tok=0, vocab_weights=tok_idf,
                               reduction='mean')

# %% ../nbs/20-nar-training-pipeline-for-distilbert-Copy1.ipynb 31
trie = XCTrie.from_block(block)

# %% ../nbs/20-nar-training-pipeline-for-distilbert-Copy1.ipynb 32
learn = XCLearner(
    model=model, 
    args=args,
    trie=trie,
    train_dataset=block.train.dset,
    eval_dataset=test_dset,
    data_collator=block.collator,
    compute_metrics=metric,
)

# %% ../nbs/20-nar-training-pipeline-for-distilbert-Copy1.ipynb 33
learn.train()
