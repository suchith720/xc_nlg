# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/77-distillation-for-wikiseealso-with-ramen.ipynb.

# %% auto 0
__all__ = ['pkl_dir', 'pkl_file', 'lco_meta', 'smp_features', 'args', 'model_output', 'm_teacher', 'bsz', 'm_student', 'model',
           'metric', 'learn']

# %% ../nbs/77-distillation-for-wikiseealso-with-ramen.ipynb 2
import os,torch, torch.multiprocessing as mp, pickle, numpy as np
from transformers import DistilBertConfig

from xcai.basics import *
from xcai.models.PPP0XX import DBT021
from xcai.models.distillation import DTL002,TCH001

# %% ../nbs/77-distillation-for-wikiseealso-with-ramen.ipynb 4
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
os.environ['WANDB_PROJECT']='xc-nlg_69-distillation-for-wikiseealso'

# %% ../nbs/77-distillation-for-wikiseealso-with-ramen.ipynb 6
pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'
pkl_file = f'{pkl_dir}/processed/wikiseealso_data-meta_distilbert-base-uncased_rm_distil-ramen-cat-2.pkl'

# %% ../nbs/77-distillation-for-wikiseealso-with-ramen.ipynb 8
with open(pkl_file, 'rb') as file: block = pickle.load(file)

# %% ../nbs/77-distillation-for-wikiseealso-with-ramen.ipynb 9
with open(f'{pkl_dir}/processed/corelations.pkl', 'rb') as file: data_corel, lbl_corel = pickle.load(file)

# %% ../nbs/77-distillation-for-wikiseealso-with-ramen.ipynb 10
from xcai.data import MetaXCDataset, XCDataset
from scipy import sparse

lco_meta = MetaXCDataset('lco', sparse.csr_matrix((block.train.dset.n_data, block.n_lbl)), lbl_corel, 
                         block.train.dset.data.lbl_info)

block.train.dset.meta['lco_meta'] = lco_meta

# %% ../nbs/77-distillation-for-wikiseealso-with-ramen.ipynb 11
smp_features=[('lbl2data|cat2lbl2data|lco2lbl2data',1,(2,1,1)), ('cat2data',1,1)]
block.collator.tfms.tfms[0].smp_features = smp_features

# %% ../nbs/77-distillation-for-wikiseealso-with-ramen.ipynb 13
args = XCLearningArguments(
    output_dir='/home/scai/phd/aiz218323/scratch/outputs/77-distillation-for-wikiseealso-with-ramen-1-0',
    logging_first_step=True,
    per_device_train_batch_size=800,
    per_device_eval_batch_size=800,
    representation_num_beams=200,
    representation_accumulation_steps=10,
    save_strategy="steps",
    evaluation_strategy="steps",
    eval_steps=3000,
    save_steps=3000,
    save_total_limit=5,
    num_train_epochs=300,
    predict_with_representation=True,
    representation_search_type='BRUTEFORCE',
    adam_epsilon=1e-6,
    warmup_steps=100,
    weight_decay=0.01,
    learning_rate=2e-4,
    group_by_cluster=True,
    num_clustering_warmup_epochs=10,
    num_cluster_update_epochs=5,
    num_cluster_size_update_epochs=25,
    clustering_type='EXPO',
    minimum_cluster_size=2,
    maximum_cluster_size=1600,
    target_indices_key='plbl2data_idx',
    target_pointer_key='plbl2data_data2ptr',
    use_encoder_parallel=True,
    max_grad_norm=None,
    fp16=True,
    label_names=['lbl2data_idx', 'lbl2data_input_ids', 'lbl2data_attention_mask', 
                 'lco2lbl2data_idx', 'lco2lbl2data_input_ids', 'lco2lbl2data_attention_mask', 
                 'cat2data_idx', 'cat2data_input_ids', 'cat2data_attention_mask'],
)

# %% ../nbs/77-distillation-for-wikiseealso-with-ramen.ipynb 14
model_output = '/home/scai/phd/aiz218323/scratch/outputs/67-ngame-ep-for-wikiseealso-with-input-concatenation-1-4'
m_teacher = TCH001.from_pretrained(f'{model_output}/teacher', n_data=block.train.dset.n_data, n_lbl=block.n_lbl)

# %% ../nbs/77-distillation-for-wikiseealso-with-ramen.ipynb 15
bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()

m_student = DBT021.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', bsz=bsz, tn_targ=1000, margin=0.3, tau=0.1, 
                                   apply_softmax=True, n_negatives=10, m_lw=0.2, data_meta_prefix='cat2data', 
                                   lbl2data_meta_prefix='lco2lbl', use_encoder_parallel=False, task_repr_type='pool', meta_repr_type='pool')

m_student.init_dr_head()

# %% ../nbs/77-distillation-for-wikiseealso-with-ramen.ipynb 16
model = DTL002(DistilBertConfig(), m_student=m_student, m_teacher=m_teacher, bsz=bsz, tn_targ=5000, margin=0.3, tau=0.1, 
               n_negatives=10, apply_softmax=True, distil_loss_weight=1.0, mse_loss_weight=0.1)

# %% ../nbs/77-distillation-for-wikiseealso-with-ramen.ipynb 18
metric = PrecRecl(block.n_lbl, block.test.data_lbl_filterer, prop=block.train.dset.data.data_lbl,
                  pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])

# %% ../nbs/77-distillation-for-wikiseealso-with-ramen.ipynb 20
learn = XCLearner(
    model=model, 
    args=args,
    train_dataset=block.train.dset,
    eval_dataset=block.test.dset,
    data_collator=block.collator,
    compute_metrics=metric,
)

# %% ../nbs/77-distillation-for-wikiseealso-with-ramen.ipynb 22
if __name__ == '__main__':
    mp.freeze_support()
    learn.train()
