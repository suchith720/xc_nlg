# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/60-nar-inference-pipeline-for-distilbert.ipynb.

# %% auto 0
__all__ = ['pkl_dir', 'pkl_file', 'encoder', 'data_dir', 'bm25', 'n_proc', 'n_data', 'bsz', 'data_text',
           'remove_multilingual_stopwords', 'preprocess_func', 'tokenize', 'tokenizer', 'get_scores']

# %% ../nbs/60-nar-inference-pipeline-for-distilbert.ipynb 4
import os, torch, torch.nn.functional as F, pickle
from tqdm.auto import tqdm
from xcai.basics import *
from xcai.models.MMM0XX import DBT007

# %% ../nbs/60-nar-inference-pipeline-for-distilbert.ipynb 6
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['WANDB_PROJECT']='xc-nlg_20-nar-training-pipeline-for-distilbert'

# %% ../nbs/60-nar-inference-pipeline-for-distilbert.ipynb 7
pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'
pkl_file = f'{pkl_dir}/processed/wikiseealso_data_distilbert-base-uncased_xcnlg_ngame.pkl'

with open(pkl_file, 'rb') as file: block = pickle.load(file)

# %% ../nbs/60-nar-inference-pipeline-for-distilbert.ipynb 9
import tiktoken, math, pickle
from stop_words import get_stop_words
from langdetect import detect
from typing import List
from rank_bm25 import BM25Okapi

# %% ../nbs/60-nar-inference-pipeline-for-distilbert.ipynb 10
def remove_multilingual_stopwords(text: str) -> str:
    # Detect the language of the text
    try: lang = detect(text)
    except: return text

    # Get the list of stop words for the detected language
    try: stop_words = set(get_stop_words(lang))
    except: return text

    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return " ".join(filtered_words)

encoder = tiktoken.encoding_for_model("gpt-4")
def preprocess_func(text: str) -> List[str]:
    lowered = text.lower()
    tokens = encoder.encode(lowered)
    return [str(token) for token in tokens]

def tokenize(text): return preprocess_func(remove_multilingual_stopwords(text))

def tokenizer(text): 
    return [tokenize(o) for o in tqdm(text, total=len(text))]

def get_scores(text):
    preds = []
    for o in tqdm(text, total=len(text)):
        sc = torch.tensor(bm25.get_scores(tokenize(o)))
        sc, idx = torch.topk(sc, 200)
        preds.append((sc,idx))
    return preds

# %% ../nbs/60-nar-inference-pipeline-for-distilbert.ipynb 11
from multiprocessing import Pool
from itertools import chain

# %% ../nbs/60-nar-inference-pipeline-for-distilbert.ipynb 13
data_dir = '/home/scai/phd/aiz218323/scratch/outputs/60-nar-inference-pipeline-for-distilbert'
with open(f'{data_dir}/wikiseealso-lbl.bow', 'rb') as file: 
    lbl_text = pickle.load(file)

# %% ../nbs/60-nar-inference-pipeline-for-distilbert.ipynb 14
bm25 = BM25Okapi(lbl_text)

# %% ../nbs/60-nar-inference-pipeline-for-distilbert.ipynb 15
n_proc, n_data = 8, block.test.dset.data.n_data
bsz = math.ceil(n_data/n_proc)

data_text = block.test.dset.data.data_info['input_text']
data_text = [data_text[i*bsz:(i+1)*bsz] for i in range(n_proc)]

# %% ../nbs/60-nar-inference-pipeline-for-distilbert.ipynb 16
with Pool(processes=n_proc) as pool:
    output = list(tqdm(pool.map(get_scores, data_text)))

data_dir = '/home/scai/phd/aiz218323/scratch/outputs/60-nar-inference-pipeline-for-distilbert'
with open(f'{data_dir}/wikiseealso-bm25-output.bow', 'wb') as file: 
    pickle.dump(lbl_text, file)
