# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/57-encoder-parallel-radga-without-layernorm-after-fusion-for-wikiseealso.ipynb.

# %% auto 0
__all__ = ['data_dir', 'pkl_dir', 'args', 'test_dset', 'metric', 'bsz', 'model', 'trie', 'learn', 'RAD003Encoder', 'RAD003']

# %% ../nbs/57-encoder-parallel-radga-without-layernorm-after-fusion-for-wikiseealso.ipynb 3
import os,sys,torch,pickle,torch.multiprocessing as mp, pickle
from xcai.basics import *
from xcai.models.radga import RAD001

# %% ../nbs/57-encoder-parallel-radga-without-layernorm-after-fusion-for-wikiseealso.ipynb 5
os.environ['CUDA_VISIBLE_DEVICES'] = '12,13'
os.environ['WANDB_PROJECT']='xc-nlg_57-encoder-parallel-radga-without-layernorm-after-fusion-for-wikiseealso'

# %% ../nbs/57-encoder-parallel-radga-without-layernorm-after-fusion-for-wikiseealso.ipynb 7
import torch.nn.functional as F
from xcai.models.radga import Encoder, RAD001, Parameters

# %% ../nbs/57-encoder-parallel-radga-without-layernorm-after-fusion-for-wikiseealso.ipynb 8
class RAD003Encoder(Encoder):
    
    def __init__(self, config, **kwargs):
        super().__init__(config, **kwargs)
        
    def fuse_meta_into_embeddings(self, embed:torch.Tensor, attention_mask:torch.Tensor, prefix:str, **kwargs):
        meta_kwargs = Parameters.from_meta_aug_prefix(prefix, **kwargs)
        
        meta_repr, weights = {}, []
        for m_key, m_args in meta_kwargs.items():
            idx = torch.where(m_args['data2ptr'] > 0)[0]
            if len(idx):
                m_input_ids, m_attention_mask = self.resize(m_args['input_ids'], m_args['attention_mask'], 
                                                            m_args['data2ptr'][idx])
                
                if self.use_noise:
                    n_input_ids, n_attention_mask = self.get_noise(m_args['input_ids'], m_args['attention_mask'], 
                                                                   m_args['data2ptr'][idx])
                    m_input_ids, m_attention_mask = self.add_noise(m_input_ids, m_attention_mask, 
                                                                   n_input_ids, n_attention_mask)
                
                m_embed = self.encode(m_input_ids, m_attention_mask)[0]
                
                m_repr = self.meta_unnormalized(m_embed, m_attention_mask)
                m_repr_mask = torch.any(m_attention_mask, dim=1)
                
                m_repr, m_repr_mask = m_repr.view(len(idx), -1, self.config.dim), m_repr_mask.view(len(idx), -1)
                
                meta_repr[m_key] = m_repr[:, :-1][m_repr_mask[:, :-1]] if self.use_noise else m_repr[m_repr_mask]
                meta_repr[m_key] = F.normalize(meta_repr[m_key], dim=1)
                
                fused_embed, w = self.cross_head(embed[idx], attention_mask[idx], m_repr, m_repr_mask, output_attentions=True)
                embed[idx] += fused_embed
                weights.append(w)
        
        return embed, weights, meta_repr
        

# %% ../nbs/57-encoder-parallel-radga-without-layernorm-after-fusion-for-wikiseealso.ipynb 9
class RAD003(RAD001):
    
    def __init__(self, config, **kwargs):
        super().__init__(config, **kwargs)
        
        self.encoder = RAD003Encoder(config, use_noise=kwargs['use_noise'], resize_length=kwargs['resize_length'])
        self.post_init()
        self.remap_post_init()
        self.init_retrieval_head()
        

# %% ../nbs/57-encoder-parallel-radga-without-layernorm-after-fusion-for-wikiseealso.ipynb 11
data_dir = '/home/aiscuser/scratch/datasets'

# %% ../nbs/57-encoder-parallel-radga-without-layernorm-after-fusion-for-wikiseealso.ipynb 13
pkl_dir = f'{data_dir}/processed/'

# %% ../nbs/57-encoder-parallel-radga-without-layernorm-after-fusion-for-wikiseealso.ipynb 15
with open(f'{pkl_dir}/wikiseealso_data-metas_distilbert-base-uncased_rm_radga-final.pkl', 'rb') as file: 
    block = pickle.load(file)

# %% ../nbs/57-encoder-parallel-radga-without-layernorm-after-fusion-for-wikiseealso.ipynb 17
args = XCLearningArguments(
    output_dir='/home/aiscuser/outputs/57-encoder-parallel-radga-without-layernorm-after-fusion-for-wikiseealso-1-0',
    logging_first_step=True,
    per_device_train_batch_size=200,
    per_device_eval_batch_size=100,
    representation_num_beams=200,
    representation_accumulation_steps=1,
    save_strategy="steps",
    evaluation_strategy='steps',
    eval_steps=1000,
    save_steps=1000,
    save_total_limit=5,
    num_train_epochs=300,
    predict_with_representation=True,
    adam_epsilon=1e-6,
    warmup_steps=100,
    weight_decay=0.01,
    learning_rate=2e-4,
    generation_num_beams=10,
    generation_length_penalty=1.5,
    predict_with_generation=True,
    representation_search_type='INDEX',
    group_by_cluster=True,
    num_clustering_warmup_epochs=10,
    num_cluster_update_epochs=5,
    num_cluster_size_update_epochs=10,
    clustering_type='EXPO',
    minimum_cluster_size=1,
    maximum_cluster_size=300,
    output_concatenation_weight=1.0,
    use_encoder_parallel=True,
    metric_for_best_model='P@1_REPR',
    target_indices_key='plbl2data_idx',
    target_pointer_key='plbl2data_data2ptr',
    fp16=True,
    label_names=['cat2data_idx', 'cat2data_input_ids', 'cat2data_attention_mask',
                 'cat2lbl2data_idx', 'cat2lbl2data_input_ids', 'cat2lbl2data_attention_mask',
                 'hlk2data_idx', 'hlk2data_input_ids', 'hlk2data_attention_mask',
                 'hlk2lbl2data_idx', 'hlk2lbl2data_input_ids', 'hlk2lbl2data_attention_mask',],
)

# %% ../nbs/57-encoder-parallel-radga-without-layernorm-after-fusion-for-wikiseealso.ipynb 18
test_dset = block.test.dset.sample(n=2000, seed=50)
metric = PrecRecl(block.n_lbl, test_dset.data.data_lbl_filterer, prop=block.train.dset.data.data_lbl,
                  pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])

# %% ../nbs/57-encoder-parallel-radga-without-layernorm-after-fusion-for-wikiseealso.ipynb 19
bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()

model = RAD003.from_pretrained('distilbert-base-uncased', num_batch_labels=5000, ignore_token=0, batch_size=bsz,
                               margin=0.3, num_negatives=5, tau=0.1, apply_softmax=True,
                               
                               data_aug_meta_prefix='hlk2data', lbl2data_aug_meta_prefix='hlk2lbl', 
                               resize_length=5000,
                               
                               gen_loss_weight=0.001, meta_loss_weight=0.3, pred_meta_prefix='cat', 
                               
                               fusion_loss_weight=0.1, tie_word_embeddings=False,
                               
                               use_fusion_loss=True, use_noise=True, use_encoder_parallel=False)

model.init_retrieval_head()
model.init_generation_head()

# %% ../nbs/57-encoder-parallel-radga-without-layernorm-after-fusion-for-wikiseealso.ipynb 20
trie = XCTrie.from_block(block)

# %% ../nbs/57-encoder-parallel-radga-without-layernorm-after-fusion-for-wikiseealso.ipynb 21
learn = XCLearner(
    model=model, 
    args=args,
    trie=trie,
    train_dataset=block.train.dset,
    eval_dataset=test_dset,
    data_collator=block.collator,
    compute_metrics=metric,
)

# %% ../nbs/57-encoder-parallel-radga-without-layernorm-after-fusion-for-wikiseealso.ipynb 27
if __name__ == '__main__':
    mp.freeze_support()
    learn.train()
    
